# Monitoring Stack Setup Summary

**Date**: 2025-10-30  
**Status**: âœ… COMPLETE - Ready for Testing

---

## What Was Implemented

### 1. ELK Stack (Logging) ðŸ“‹

**Services Deployed:**

- Elasticsearch 8.11.0 (ports 9200, 9300)
- Logstash 8.11.0 (ports 5000, 9600)
- Kibana 8.11.0 (port 5601)
- Filebeat 8.11.0

**Configuration Files:**

```
docker-compose.elk.yml                          # ELK services
infra/elk/filebeat/filebeat.yml                 # Container log collection
infra/elk/logstash/pipeline/logstash.conf       # Log processing pipeline
backend/app/core/logging_config.py              # Structured JSON logging
```

**Features:**

- âœ… Structured JSON logging with context fields (user_id, agent, session_id, processing_time_ms)
- âœ… Privacy-preserving user_id hashing
- âœ… Crisis and intervention event tagging
- âœ… Automatic Docker log shipping
- âœ… Log indexing: `ugm-aicare-YYYY.MM.DD`

---

### 2. Prometheus + Grafana (Metrics) ðŸ“ˆ

**Services Deployed:**

- Prometheus 2.48.0 (port 9090)
- Grafana 10.2.2 (port 3001, admin/admin123)
- AlertManager 0.26.0 (port 9093)
- Node Exporter 1.7.0 (port 9100)
- cAdvisor 0.47.2 (port 8080)
- Postgres Exporter 0.15.0 (port 9187)
- Redis Exporter 1.55.0 (port 9121)

**Configuration Files:**

```
docker-compose.monitoring.yml                    # Monitoring services
infra/monitoring/prometheus/prometheus.yml       # Prometheus config (6 scrape jobs)
infra/monitoring/prometheus/alert_rules.yml      # 15 alert rules
infra/monitoring/alertmanager/alertmanager.yml   # Slack integration
infra/monitoring/grafana/provisioning/           # Grafana datasources
backend/app/core/metrics.py                      # 50+ custom metrics
```

**Features:**

- âœ… 50+ custom Prometheus metrics for mental health platform
- âœ… 15 automated alerts (6 Critical, 9 Warning, 2 Info)
- âœ… Slack integration support
- âœ… System, container, database, and cache exporters
- âœ… /metrics endpoints exposed in backend

---

### 3. Deployment Scripts ðŸš€

**Updated Scripts:**

```
dev.sh                              # Integrated monitoring commands
monitoring.sh                       # Standalone monitoring script
scripts/start-monitoring.sh         # Helper script to start monitoring
scripts/stop-monitoring.sh          # Helper script to stop monitoring
```

**New Commands Available:**

```bash
# Option 1: Start everything
./dev.sh up-all                      # App + Monitoring

# Option 2: Separate deployment
./dev.sh up                          # Start app
./dev.sh monitoring start            # Start monitoring separately

# Option 3: Standalone monitoring
./monitoring.sh start                # Monitoring only

# Management commands
./dev.sh monitoring stop             # Stop monitoring
./dev.sh monitoring restart          # Restart monitoring
./dev.sh monitoring logs kibana      # View Kibana logs
./dev.sh monitoring status           # Check health
./dev.sh down-all                    # Stop everything

# Standalone commands
./monitoring.sh stop                 # Stop monitoring
./monitoring.sh logs prometheus      # View Prometheus logs
./monitoring.sh status               # Check health
./monitoring.sh clean                # Remove containers and volumes
./monitoring.sh urls                 # Display access URLs
```

---

### 4. Backend Instrumentation ðŸ”§

**Modified Files:**

```
backend/app/main.py                 # Added structured logging + metrics endpoints
backend/requirements.txt            # Added prometheus-client, prometheus-fastapi-instrumentator
```

**New Endpoints:**

- `GET /metrics` - Custom Prometheus metrics
- `GET /metrics/fastapi` - Default FastAPI metrics

**Metric Categories (50+ metrics):**

- HTTP: Request rate, response time, error rate
- Agents: Processing time, invocations, errors (STA, SCA, SDA, IA)
- LLM: API calls, latency, token usage
- Mental Health: Crisis escalations, intervention plans, completion rates
- Users: Active users, session duration, retention
- Counselors: Response time, case load
- Database: Query duration, connection pool
- Cache: Hit rate, misses

**Metric Decorators:**

```python
@track_agent_metrics("STA")
@track_tool_metrics("get_user_profile")
@track_db_metrics
@track_llm_metrics("gemini-2.0-flash")
```

---

### 5. Documentation ðŸ“š

**Created Documentation:**

```
docs/PRODUCTION_MONITORING.md        # Complete monitoring guide (400+ lines)
docs/MONITORING_QUICK_REFERENCE.md   # Quick reference and commands
docs/MONITORING_IMPLEMENTATION.md    # Implementation details and architecture
MONITORING_SETUP_SUMMARY.md          # This file
```

**Updated Documentation:**

```
README.md                            # Added "Production Monitoring Stack" section
backend/README.md                    # (Pending) Add monitoring section
```

---

## Quick Start Guide

### Step 1: Start the Monitoring Stack

**Option A: Start Everything (Recommended)**

```bash
cd /d/Astaga\ Ngoding/Github/UGM-AICare
./dev.sh up-all
```

**Option B: Start Monitoring Only**

```bash
./monitoring.sh start
```

### Step 2: Wait for Services to Start (60 seconds)

```bash
sleep 60
```

### Step 3: Verify Services

```bash
# Check monitoring service status
./dev.sh monitoring status

# Or with standalone script
./monitoring.sh status
```

### Step 4: Access Monitoring Dashboards

- **Kibana** (Logs): <http://localhost:5601>
- **Grafana** (Metrics): <http://localhost:3001> (admin/admin123)
- **Prometheus** (Metrics Query): <http://localhost:9090>
- **AlertManager** (Alerts): <http://localhost:9093>
- **Backend Metrics**: <http://localhost:8000/metrics>

### Step 5: Set Up Kibana Index Patterns

1. Open Kibana: <http://localhost:5601>
2. Go to: **Management â†’ Stack Management â†’ Index Patterns**
3. Click: **Create index pattern**
4. Enter pattern: `ugm-aicare-*`
5. Select timestamp field: `@timestamp`
6. Click: **Create index pattern**
7. Go to **Discover** to view logs

**Useful Kibana Queries:**

```
app.level: ERROR                    # Error logs
tags: crisis                        # Crisis events
agent_name: "STA"                   # Safety Triage Agent logs
app.intervention_plan_id: *         # Intervention plan logs
```

### Step 6: Explore Grafana Dashboards

1. Open Grafana: <http://localhost:3001> (admin/admin123)
2. Go to: **Explore**
3. Select datasource: **Prometheus**
4. Try example queries:

   ```promql
   # Request rate
   rate(http_requests_total[5m])
   
   # Response time P95
   histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))
   
   # Agent processing time
   rate(agent_processing_time_seconds_sum[5m]) / rate(agent_processing_time_seconds_count[5m])
   
   # Crisis escalations per hour
   rate(crisis_escalations_total[1h])
   
   # Active users
   active_users
   ```

---

## Health Checks

**Verify all services are healthy:**

```bash
# Elasticsearch cluster health
curl http://localhost:9200/_cluster/health

# Prometheus targets
curl http://localhost:9090/api/v1/targets | jq '.data.activeTargets[] | {job: .labels.job, health: .health}'

# Grafana health
curl http://localhost:3001/api/health

# Backend metrics endpoint
curl http://localhost:8000/metrics | head -50

# AlertManager status
curl http://localhost:9093/api/v2/status | jq
```

**Expected Output:**

- Elasticsearch: `"status":"green"` or `"status":"yellow"`
- Prometheus: All targets should have `"health":"up"`
- Grafana: `{"commit":"...", "database":"ok", "version":"10.2.2"}`
- Backend metrics: Should return Prometheus metrics format

---

## Alerting Configuration

### Current Alert Rules (15 alerts)

**Critical Alerts (6):**

1. `HighErrorRate` - HTTP error rate > 5%
2. `BackendServiceDown` - Backend unreachable
3. `DatabaseConnectionPoolLow` - < 5 DB connections available
4. `CrisisEscalationBacklog` - > 10 pending crisis escalations
5. `HighMemoryUsage` - Memory > 90%
6. `LowDiskSpace` - Disk < 10% free

**Warning Alerts (9):**

1. `SlowResponseTime` - P95 response time > 2s
2. `SlowAgentProcessing` - P95 agent processing > 5s
3. `HighCPUUsage` - CPU > 80%
4. `SlowLLMAPI` - P95 LLM latency > 10s
5. `HighLLMTokenUsage` - Hourly tokens > 100,000
6. `LowInterventionPlanCompletion` - Completion rate < 50%
7. `HighUserActivity` - > 500 active users
8. `UnusualCrisisPattern` - 3x baseline crisis escalations
9. (Additional warning rules)

**Info Alerts (2):**

1. `HighUserActivity` - Monitoring user surge
2. `UnusualCrisisPattern` - Pattern detection

### Configure Slack Integration

**Edit alertmanager.yml:**

```bash
# Open alertmanager config
nano infra/monitoring/alertmanager/alertmanager.yml

# Replace placeholder with your Slack webhook
# Line 7: api_url: 'YOUR_SLACK_WEBHOOK_URL_HERE'
# Replace with: https://hooks.slack.com/services/YOUR/WEBHOOK/URL
```

**Restart AlertManager:**

```bash
docker compose -f docker-compose.monitoring.yml restart alertmanager
```

**Test Alert:**

```bash
# Simulate high error rate (100 failed requests)
for i in {1..100}; do curl http://localhost:8000/nonexistent; done

# Check if alert fired
curl http://localhost:9090/alerts | jq '.data.alerts[] | select(.labels.alertname=="HighErrorRate")'

# Check AlertManager
curl http://localhost:9093/api/v2/alerts | jq
```

---

## Resource Requirements

**Minimum System Resources:**

- **CPU**: 4 cores (2 for app + 2 for monitoring)
- **RAM**: 8 GB (4 GB for app + 4 GB for monitoring)
- **Disk**: 20 GB (10 GB for app + 10 GB for logs/metrics)

**Production Recommendations:**

- **CPU**: 8+ cores
- **RAM**: 16+ GB
- **Disk**: 100+ GB SSD (with 30-day log retention)

**Docker Compose Resource Limits:**

```yaml
# ELK Stack
elasticsearch:
  deploy:
    resources:
      limits:
        memory: 2G
      reservations:
        memory: 1G

# Prometheus
prometheus:
  deploy:
    resources:
      limits:
        memory: 1G
```

---

## Production Deployment Checklist

### Pre-Deployment

- [ ] Review resource requirements
- [ ] Allocate sufficient disk space (100+ GB recommended)
- [ ] Configure firewall rules
- [ ] Set up SSL/TLS certificates for Grafana and Kibana
- [ ] Prepare Slack webhook URL

### Configuration

- [ ] Update `infra/monitoring/alertmanager/alertmanager.yml` with Slack webhook
- [ ] Change Grafana admin password (default: admin/admin123)
- [ ] Set Elasticsearch retention policy: 30 days
- [ ] Set Prometheus retention: `--storage.tsdb.retention.time=30d`
- [ ] Configure log rotation for disk management
- [ ] Review and adjust alert thresholds in `alert_rules.yml`

### Post-Deployment

- [ ] Create Kibana index patterns (`ugm-aicare-*`)
- [ ] Import Grafana dashboards
- [ ] Test alert delivery (Slack notifications)
- [ ] Set up automated Elasticsearch backups
- [ ] Configure monitoring access control (if exposed publicly)
- [ ] Document runbooks for common incidents
- [ ] Train team on monitoring tools

### Security

- [ ] Never expose Elasticsearch directly to the internet
- [ ] Use authentication for Grafana and Kibana
- [ ] Secure AlertManager webhook URLs
- [ ] Implement network policies (Docker networks)
- [ ] Enable audit logging for monitoring access
- [ ] Rotate Grafana admin credentials

---

## Troubleshooting

### Services Won't Start

**Problem**: Docker containers fail to start

```bash
# Check Docker logs
./dev.sh monitoring logs elasticsearch
./dev.sh monitoring logs prometheus

# Check if ports are already in use
netstat -tulpn | grep -E '5601|9090|3001|9200'

# Check Docker disk space
docker system df
```

**Solution**: Stop conflicting services or change ports in docker-compose files

---

### Elasticsearch Health is Red

**Problem**: `curl http://localhost:9200/_cluster/health` returns `"status":"red"`

```bash
# Check Elasticsearch logs
docker compose -f docker-compose.elk.yml logs elasticsearch

# Check disk space (Elasticsearch requires > 10% free)
df -h

# Reset Elasticsearch (WARNING: Deletes all logs)
docker compose -f docker-compose.elk.yml down -v
docker compose -f docker-compose.elk.yml up -d
```

---

### No Metrics in Prometheus

**Problem**: Prometheus shows no data for `http_requests_total`

```bash
# Check if backend is exposing metrics
curl http://localhost:8000/metrics

# Check Prometheus targets
curl http://localhost:9090/api/v1/targets | jq '.data.activeTargets[] | select(.labels.job=="backend")'

# Check if backend is reachable from Prometheus container
docker exec -it $(docker ps -q -f name=prometheus) wget -O- http://backend:8000/metrics
```

**Solution**: Ensure backend is running and network `aicare_dev` exists

---

### Alerts Not Firing

**Problem**: Simulated errors don't trigger alerts

```bash
# Check alert rules are loaded
curl http://localhost:9090/api/v1/rules | jq '.data.groups[] | .name'

# Check if alert is pending/firing
curl http://localhost:9090/alerts

# Check AlertManager is receiving alerts
curl http://localhost:9093/api/v2/alerts | jq

# Verify Slack webhook is configured
docker exec -it $(docker ps -q -f name=alertmanager) cat /etc/alertmanager/alertmanager.yml
```

**Solution**: Wait for evaluation interval (15-60s), verify Slack webhook URL

---

### High Resource Usage

**Problem**: Elasticsearch or Prometheus consuming too much memory

```bash
# Check resource usage
docker stats

# Reduce Elasticsearch memory (edit docker-compose.elk.yml)
# ES_JAVA_OPTS=-Xms512m -Xmx512m

# Reduce Prometheus retention
# --storage.tsdb.retention.time=15d
```

---

## Next Steps

### Immediate Actions

1. **Test the monitoring stack:**

   ```bash
   ./dev.sh up-all
   sleep 60
   ./dev.sh monitoring status
   ```

2. **Access all dashboards and verify they load:**
   - Kibana: <http://localhost:5601>
   - Grafana: <http://localhost:3001>
   - Prometheus: <http://localhost:9090>

3. **Create Kibana index patterns** as described above

4. **Configure Slack webhook** in alertmanager.yml

5. **Test alert delivery** by simulating errors

### Short-Term (Next Sprint)

- [ ] Add metric decorators to agent adapters (`app/agents/aika/agent_adapters.py`)
- [ ] Create custom Grafana dashboards for mental health metrics
- [ ] Implement automated Elasticsearch backups
- [ ] Set up log retention policies (30 days)
- [ ] Create incident response runbooks

### Long-Term (Production Readiness)

- [ ] Implement distributed tracing (Jaeger/Zipkin)
- [ ] Add real-time log streaming to Slack for critical errors
- [ ] Set up PagerDuty integration for critical alerts
- [ ] Create SLA dashboards (uptime, response time, error budget)
- [ ] Implement cost optimization for log storage
- [ ] Add anomaly detection for crisis patterns
- [ ] Create executive dashboards for stakeholders

---

## Cost Estimation

**Development Environment (Local):**

- **Cost**: $0 (uses local resources)
- **Resource Usage**: 4 GB RAM + 10 GB disk for monitoring

**Production Environment (VM/Cloud):**

- **Small Scale** (< 1000 users):
  - VM: 4 CPU + 16 GB RAM = ~$50-80/month
  - Storage: 100 GB SSD = ~$10/month
  - **Total**: ~$60-90/month

- **Medium Scale** (1000-10000 users):
  - VM: 8 CPU + 32 GB RAM = ~$150-200/month
  - Storage: 500 GB SSD = ~$50/month
  - **Total**: ~$200-250/month

- **Large Scale** (> 10000 users):
  - Consider managed services (Elastic Cloud, Grafana Cloud)
  - Estimated: $500-1000/month

---

## References

- **ELK Stack Documentation**: <https://www.elastic.co/guide/index.html>
- **Prometheus Documentation**: <https://prometheus.io/docs/>
- **Grafana Documentation**: <https://grafana.com/docs/>
- **Docker Compose Documentation**: <https://docs.docker.com/compose/>

---

## Support

For issues or questions:

1. Check troubleshooting section above
2. Review full documentation in `docs/PRODUCTION_MONITORING.md`
3. Check Docker logs: `./dev.sh monitoring logs [service]`
4. Open GitHub issue with logs and error messages

---

**Implementation Date**: 2025-10-30  
**Version**: 1.0.0  
**Status**: âœ… Ready for Testing
