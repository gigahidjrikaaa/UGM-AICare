# LLM Optimization Options - Faster Backend Build

## üö® Current Problem

**Issue:** Backend build is very slow due to heavy ML dependencies

### Current Implementation Analysis

#### Dependency Sizes

| Package | Version | Download Size | Installed Size |
|---------|---------|---------------|----------------|
| **torch** | 2.6.0+cpu | ~206 MB | ~800+ MB |
| **transformers** | 4.57.1 | ~8 MB | ~40 MB |
| **sentence-transformers** | 5.1.2 | ~50 MB | ~60 MB |
| numpy | 2.3.4 | ~13 MB | ~60 MB |
| scikit-learn | 1.7.2 | ~10 MB | ~40 MB |
| scipy | 1.16.2 | ~30 MB | ~120 MB |
| **Model weights** | MiniLM-L12-v2 | ~471 MB | ~471 MB |
| **TOTAL** | - | **~788 MB** | **~1.6 GB** |

#### Build Time Breakdown

```
pip install torch               ‚Üí 2-5 minutes (206 MB download)
pip install sentence-transformers ‚Üí 30-60 seconds
pip install transformers        ‚Üí 20-40 seconds  
Model download (first run)      ‚Üí 3-5 minutes (471 MB)
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
TOTAL FIRST BUILD              ‚Üí 6-11 minutes
TOTAL SUBSEQUENT BUILDS        ‚Üí 3-6 minutes (no model download)
```

#### Performance

- **Inference Time:** 50-100ms per message
- **Memory Usage:** ~500 MB RAM
- **Accuracy:** 85-95%
- **CPU Usage:** 5-10% during inference

---

## ‚úÖ Recommended Solutions (Ranked Best to Worst)

### **Option 1: Switch to ONNX Runtime** ‚≠ê **BEST OPTION**

Replace PyTorch with ONNX Runtime for 3-5x faster inference and much smaller footprint.

#### Benefits

- ‚úÖ **70-80% smaller dependencies** (removes 800+ MB PyTorch)
- ‚úÖ **3-5x faster inference** (30-50ms vs 50-100ms)
- ‚úÖ **Same or better accuracy** (identical model, optimized runtime)
- ‚úÖ **Faster build times** (2-3 minutes vs 6-11 minutes)
- ‚úÖ **Cross-platform optimization** (SIMD, AVX2 instructions)
- ‚úÖ **Minimal code changes** (~20 lines)

#### Dependency Changes

```diff
# Remove
- torch==2.6.0  # 206 MB download, 800+ MB installed
- sentence-transformers>=2.2.0
- transformers>=4.41.0

# Add  
+ onnxruntime>=1.16.0  # ~15 MB download, ~60 MB installed
+ optimum[onnxruntime]>=1.14.0  # ~5 MB, HuggingFace optimization library
+ tokenizers>=0.15.0  # ~10 MB (faster than transformers tokenizer)
```

#### New Sizes

| Package | Download | Installed | Change |
|---------|----------|-----------|--------|
| onnxruntime | ~15 MB | ~60 MB | -785 MB vs torch |
| optimum | ~5 MB | ~20 MB | - |
| tokenizers | ~10 MB | ~30 MB | - |
| **TOTAL** | **~30 MB** | **~110 MB** | **-680 MB (86% reduction)** |

#### Build Time Impact

```
Before: 6-11 minutes (first), 3-6 minutes (subsequent)
After:  1-2 minutes (first), 30-60 seconds (subsequent)
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
IMPROVEMENT: 75-85% faster builds
```

#### Implementation Steps

**1. Export model to ONNX format (one-time setup)**

```python
# scripts/export_model_to_onnx.py
from optimum.onnxruntime import ORTModelForFeatureExtraction
from transformers import AutoTokenizer

model_name = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
output_dir = "./models/onnx/minilm-l12-v2"

# Export to ONNX
model = ORTModelForFeatureExtraction.from_pretrained(model_name, export=True)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Save optimized model
model.save_pretrained(output_dir)
tokenizer.save_pretrained(output_dir)

print(f"‚úÖ Model exported to {output_dir}")
print(f"Model size: {sum(f.stat().st_size for f in Path(output_dir).rglob('*'))/1e6:.1f} MB")
```

**2. Update ML classifier to use ONNX**

```python
# backend/app/agents/sta/ml_classifier_onnx.py
from optimum.onnxruntime import ORTModelForFeatureExtraction
from transformers import AutoTokenizer
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

class ONNXSemanticClassifier:
    """ONNX-optimized semantic classifier (3-5x faster than PyTorch)."""
    
    def __init__(self, model_path: str = "paraphrase-multilingual-MiniLM-L12-v2"):
        self.model = None
        self.tokenizer = None
        self.model_path = model_path
        self._load_model()
    
    def _load_model(self) -> bool:
        try:
            logger.info(f"Loading ONNX model: {self.model_path}")
            
            # Load ONNX model (much faster than PyTorch)
            self.model = ORTModelForFeatureExtraction.from_pretrained(
                self.model_path,
                provider="CPUExecutionProvider"  # Use CPU optimizations
            )
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)
            
            # Pre-compute crisis embeddings
            for severity, examples in CRISIS_EXAMPLES.items():
                embeddings = self._encode(examples)
                self.crisis_embeddings[severity] = embeddings
            
            logger.info("‚úÖ ONNX classifier ready (optimized)")
            return True
        except Exception as e:
            logger.error(f"Failed to load ONNX model: {e}")
            return False
    
    def _encode(self, texts: list[str]) -> np.ndarray:
        """Encode texts to embeddings using ONNX model."""
        # Tokenize
        inputs = self.tokenizer(
            texts,
            padding=True,
            truncation=True,
            max_length=128,
            return_tensors="pt"
        )
        
        # Run ONNX inference (3-5x faster than PyTorch)
        outputs = self.model(**inputs)
        embeddings = outputs.last_hidden_state[:, 0, :].detach().numpy()
        
        return embeddings
    
    async def classify(self, payload: STAClassifyRequest) -> STAClassifyResponse:
        """Same interface as current classifier, but faster."""
        user_embedding = self._encode([payload.text])[0]
        
        # Compute similarities (same logic as before)
        similarity_scores = {}
        for severity, embeddings in self.crisis_embeddings.items():
            similarities = cosine_similarity([user_embedding], embeddings)[0]
            similarity_scores[severity] = float(np.max(similarities))
        
        # Threshold-based classification (unchanged)
        # ... rest of classification logic ...
```

**3. Update requirements.txt**

```diff
# backend/requirements.txt

- # AI and ML
- torch==2.6.0  # Updated to fix CVE-2025-32434 (use --extra-index-url for CPU version)
- tiktoken==0.5.1
- openai==1.3.5
- anthropic==0.5.0
- 
- google-generativeai
- langgraph
- 
- # Semantic Similarity for Crisis Detection
- sentence-transformers>=2.2.0  # For ML-based crisis classification

+ # AI and ML
+ tiktoken==0.5.1
+ openai==1.3.5
+ anthropic==0.5.0
+ 
+ google-generativeai
+ langgraph
+ 
+ # ONNX Runtime for Crisis Detection (3-5x faster, 80% smaller than PyTorch)
+ onnxruntime>=1.16.0  # Optimized inference runtime
+ optimum[onnxruntime]>=1.14.0  # HuggingFace ONNX optimization
+ tokenizers>=0.15.0  # Fast tokenization
```

**4. One-time model export**

```bash
# Run once to export model to ONNX
cd backend
python scripts/export_model_to_onnx.py

# Output: models/onnx/minilm-l12-v2/
#   - model.onnx (optimized model, ~180 MB vs 471 MB PyTorch)
#   - tokenizer files
```

**5. Update service to use ONNX classifier**

```python
# backend/app/agents/sta/service.py

def get_safety_triage_service(session: AsyncSession):
    rule_classifier = SafetyTriageClassifier()
    
    try:
        # Use ONNX classifier instead of PyTorch
        from app.agents.sta.ml_classifier_onnx import ONNXSemanticClassifier
        semantic_classifier = ONNXSemanticClassifier()
        classifier = HybridClassifier(rule_classifier, semantic_classifier)
    except Exception as e:
        logger.warning(f"ONNX classifier failed, using rule-based: {e}")
        classifier = rule_classifier
    
    return SafetyTriageService(classifier=classifier, session=session)
```

#### Performance Comparison

| Metric | Current (PyTorch) | ONNX Runtime | Improvement |
|--------|-------------------|--------------|-------------|
| Dependencies | 788 MB | 30 MB | **96% smaller** |
| Installed Size | 1.6 GB | 110 MB | **93% smaller** |
| Build Time | 6-11 min | 1-2 min | **75-85% faster** |
| Inference Time | 50-100ms | 15-30ms | **3-5x faster** |
| Model Size | 471 MB | 180 MB | **62% smaller** |
| Memory Usage | 500 MB | 200 MB | **60% less** |
| Accuracy | 85-95% | 85-95% | **Same** |

#### Production Deployment

```dockerfile
# Dockerfile (optimized)
FROM python:3.11-slim

# Install dependencies (much faster)
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy pre-exported ONNX model
COPY models/onnx/minilm-l12-v2 /app/models/onnx/minilm-l12-v2

COPY . .
CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000"]

# Build time: 2-3 minutes vs 10-15 minutes with PyTorch
```

---

### **Option 2: Use Lighter Model (all-MiniLM-L6-v2)** ‚≠ê‚≠ê

Keep sentence-transformers but use smaller model.

#### Benefits

- ‚úÖ **60% faster inference** (20-40ms vs 50-100ms)
- ‚úÖ **70% smaller model** (90 MB vs 471 MB)
- ‚ö†Ô∏è **Slightly lower accuracy** (80-90% vs 85-95%)
- ‚ö†Ô∏è **English-only** (no Indonesian support)

#### Changes

```diff
# backend/app/agents/sta/ml_classifier.py

class SemanticCrisisClassifier:
-   def __init__(self, model_name: str = "paraphrase-multilingual-MiniLM-L12-v2"):
+   def __init__(self, model_name: str = "all-MiniLM-L6-v2"):
```

#### Trade-offs

| Metric | Current (L12-v2) | Lighter (L6-v2) | Change |
|--------|------------------|-----------------|--------|
| Model Size | 471 MB | 90 MB | -381 MB |
| Inference | 50-100ms | 20-40ms | 2-3x faster |
| Accuracy | 85-95% | 80-90% | -5% |
| Languages | 50+ | English only | ‚ùå No Indonesian |

**Verdict:** ‚ùå Not recommended due to loss of Indonesian support (critical for UGM)

---

### **Option 3: Quantized ONNX Model** ‚≠ê‚≠ê‚≠ê‚≠ê

Combine ONNX with INT8 quantization for maximum efficiency.

#### Benefits

- ‚úÖ **95% smaller dependencies** (vs PyTorch)
- ‚úÖ **5-8x faster inference** (10-20ms)
- ‚úÖ **4x smaller model** (45 MB vs 180 MB ONNX, vs 471 MB PyTorch)
- ‚ö†Ô∏è **Minimal accuracy loss** (~2-3%, still 82-92%)

#### Implementation

```python
# scripts/export_quantized_onnx.py
from optimum.onnxruntime import ORTModelForFeatureExtraction, ORTQuantizer
from optimum.onnxruntime.configuration import AutoQuantizationConfig

model_name = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"

# Export to ONNX
model = ORTModelForFeatureExtraction.from_pretrained(model_name, export=True)

# Quantize to INT8 (4x smaller, 2x faster)
quantizer = ORTQuantizer.from_pretrained(model)
qconfig = AutoQuantizationConfig.avx512_vnni(is_static=False, per_channel=True)

quantized_model = quantizer.quantize(
    save_dir="./models/onnx/minilm-l12-v2-int8",
    quantization_config=qconfig
)

print("‚úÖ Quantized model ready")
print(f"Size: {sum(f.stat().st_size for f in Path('./models/onnx/minilm-l12-v2-int8').rglob('*'))/1e6:.1f} MB")
```

#### Performance

| Metric | Current | Quantized ONNX | Improvement |
|--------|---------|----------------|-------------|
| Model Size | 471 MB | 45 MB | **90% smaller** |
| Inference | 50-100ms | 10-20ms | **5-8x faster** |
| Accuracy | 85-95% | 82-92% | -3% acceptable |
| Memory | 500 MB | 150 MB | **70% less** |

**Verdict:** ‚úÖ Best option if you can tolerate 3% accuracy loss

---

### **Option 4: TensorFlow Lite** ‚≠ê‚≠ê

Replace PyTorch with TensorFlow Lite (mobile-optimized).

#### Benefits

- ‚úÖ **50% smaller dependencies** (vs PyTorch)
- ‚úÖ **2-3x faster inference**
- ‚ö†Ô∏è **Requires model conversion**
- ‚ö†Ô∏è **Less mature ecosystem for sentence embeddings**

#### Dependency Changes

```diff
- torch==2.6.0
- sentence-transformers>=2.2.0
+ tensorflow-lite>=2.14.0  # ~100 MB vs 800+ MB PyTorch
+ sentence-transformers-tflite  # Third-party wrapper
```

**Verdict:** ‚ö†Ô∏è Less mature, ONNX is better supported

---

### **Option 5: Remove ML Entirely (Rule-Based Only)** ‚≠ê

Simplify to pure rule-based classification.

#### Benefits

- ‚úÖ **0 MB dependencies** (remove all ML libraries)
- ‚úÖ **Instant inference** (<1ms)
- ‚úÖ **Fastest builds** (seconds)
- ‚ùå **Lower accuracy** (60-70% vs 85-95%)
- ‚ùå **Misses semantic variations**

#### Changes

```diff
# backend/requirements.txt
- torch==2.6.0
- sentence-transformers>=2.2.0
- transformers>=4.41.0
- numpy>=1.24.0
- scipy>=1.10.0
- scikit-learn>=1.3.0
```

```python
# backend/app/agents/sta/service.py
def get_safety_triage_service(session: AsyncSession):
    # Only use rule-based classifier
    classifier = SafetyTriageClassifier()
    return SafetyTriageService(classifier=classifier, session=session)
```

#### Trade-offs

- **Build time:** Seconds (excellent)
- **Accuracy:** 60-70% (poor for mental health)
- **False negatives:** 15-20% (dangerous - misses crises)

**Verdict:** ‚ùå Not recommended - accuracy too low for safety-critical application

---

### **Option 6: Pre-built Docker Image with Cached Models** ‚≠ê‚≠ê‚≠ê

Keep current implementation but optimize deployment.

#### Strategy

Build Docker image with pre-installed dependencies and cached models.

```dockerfile
# Dockerfile.cached
FROM python:3.11-slim AS builder

# Install dependencies in builder stage
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Pre-download model
RUN python -c "from sentence_transformers import SentenceTransformer; \
    SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')"

FROM python:3.11-slim

# Copy pre-installed dependencies
COPY --from=builder /usr/local/lib/python3.11/site-packages /usr/local/lib/python3.11/site-packages
COPY --from=builder /root/.cache/huggingface /root/.cache/huggingface

COPY . .
CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

```bash
# Build once, push to registry
docker build -t ugm-aicare-backend:latest -f Dockerfile.cached .
docker push ghcr.io/gigahidjrikaaa/ugm-aicare-backend:latest

# Subsequent deployments: instant (pull cached image)
docker pull ghcr.io/gigahidjrikaaa/ugm-aicare-backend:latest
```

#### Benefits

- ‚úÖ **First build:** 6-11 minutes (one-time)
- ‚úÖ **Subsequent deploys:** 30-60 seconds (pull cached image)
- ‚úÖ **No code changes**
- ‚ö†Ô∏è **Larger image size** (~2 GB with dependencies + model)

**Verdict:** ‚úÖ Good short-term solution if you can't change code immediately

---

## üìä Comprehensive Comparison

| Option | Build Time | Dependency Size | Inference Speed | Accuracy | Code Changes | Recommendation |
|--------|-----------|-----------------|-----------------|----------|--------------|----------------|
| **Current (PyTorch)** | 6-11 min | 788 MB | 50-100ms | 85-95% | - | ‚ùå Slow builds |
| **1. ONNX Runtime** | 1-2 min | 30 MB | 15-30ms | 85-95% | Minimal (~20 lines) | ‚úÖ **BEST** |
| **2. Lighter Model** | 6-11 min | 788 MB | 20-40ms | 80-90% | 1 line | ‚ùå No Indonesian |
| **3. Quantized ONNX** | 1-2 min | 30 MB | 10-20ms | 82-92% | Minimal | ‚úÖ **BEST IF OK WITH -3% ACC** |
| **4. TensorFlow Lite** | 3-5 min | 400 MB | 30-60ms | 85-95% | Moderate | ‚ö†Ô∏è Less mature |
| **5. Rule-Based Only** | Seconds | 0 MB | <1ms | 60-70% | Remove ML code | ‚ùå Too low accuracy |
| **6. Cached Docker** | 30-60s (cached) | 788 MB | 50-100ms | 85-95% | None | ‚úÖ Quick fix |

---

## üéØ Final Recommendation

### **Implement Option 1 (ONNX Runtime) + Option 6 (Cached Docker)**

**Short-term (Immediate):**

1. Build cached Docker image with current implementation
2. Push to container registry
3. Deploy from cached image (30-60 second deploys)

**Medium-term (1-2 weeks):**

1. Migrate to ONNX Runtime
2. Export model to ONNX format
3. Update ml_classifier to use ONNX
4. Update requirements.txt
5. Rebuild Docker image (now 1-2 minute builds)

**Long-term (Optional):**

1. Implement INT8 quantization for 5-8x speedup
2. Monitor accuracy (should stay 82-92%)
3. Fine-tune thresholds if needed

### **Expected Results**

**After Short-term Fix:**

```
Current:  6-11 minute builds
After:    30-60 second deploys (from cache)
```

**After Medium-term Migration:**

```
Dependencies: 788 MB ‚Üí 30 MB (96% reduction)
Build time:   6-11 min ‚Üí 1-2 min (75-85% faster)
Inference:    50-100ms ‚Üí 15-30ms (3-5x faster)
Memory:       500 MB ‚Üí 200 MB (60% less)
Accuracy:     Same (85-95%)
```

**After Long-term Optimization:**

```
Model size:   471 MB ‚Üí 45 MB (90% reduction)
Inference:    15-30ms ‚Üí 10-20ms (5-8x vs current)
Accuracy:     82-92% (acceptable for mental health)
```

---

## üõ†Ô∏è Implementation Guide

### Step 1: Short-term Fix (Today)

```bash
# 1. Create cached Dockerfile
cat > Dockerfile.cached << 'EOF'
FROM python:3.11-slim AS builder
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt
RUN python -c "from sentence_transformers import SentenceTransformer; \
    SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')"

FROM python:3.11-slim
COPY --from=builder /usr/local/lib/python3.11/site-packages /usr/local/lib/python3.11/site-packages
COPY --from=builder /root/.cache/huggingface /root/.cache/huggingface
COPY . .
CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000"]
EOF

# 2. Build and push
docker build -t ugm-aicare-backend:cached -f Dockerfile.cached .
docker push ghcr.io/gigahidjrikaaa/ugm-aicare-backend:cached

# 3. Subsequent deploys (instant)
docker pull ghcr.io/gigahidjrikaaa/ugm-aicare-backend:cached
docker run -p 8000:8000 ghcr.io/gigahidjrikaaa/ugm-aicare-backend:cached
```

### Step 2: Medium-term Migration (Next Sprint)

```bash
# 1. Install ONNX tools (one-time)
pip install optimum[onnxruntime] onnxruntime

# 2. Export model to ONNX
python scripts/export_model_to_onnx.py
# Output: models/onnx/minilm-l12-v2/ (180 MB)

# 3. Update requirements.txt (see Option 1 above)

# 4. Update ml_classifier.py (see implementation above)

# 5. Test accuracy
python scripts/test_sta_ml.py
# Should maintain 85-95% accuracy

# 6. Rebuild and deploy
docker build -t ugm-aicare-backend:onnx .
docker push ghcr.io/gigahidjrikaaa/ugm-aicare-backend:onnx
```

### Step 3: Long-term Optimization (Optional)

```bash
# 1. Quantize model to INT8
python scripts/export_quantized_onnx.py
# Output: models/onnx/minilm-l12-v2-int8/ (45 MB)

# 2. Update classifier to use quantized model

# 3. Benchmark and validate accuracy
python scripts/benchmark_quantized.py
# Target: >82% accuracy maintained

# 4. Deploy if accuracy acceptable
```

---

## üìà ROI Analysis

### Development Effort

| Phase | Effort | Timeline |
|-------|--------|----------|
| Short-term (Docker cache) | 1-2 hours | Today |
| Medium-term (ONNX) | 1-2 days | Next sprint |
| Long-term (Quantization) | 2-3 days | Optional |

### Business Impact

| Metric | Before | After ONNX | Impact |
|--------|--------|------------|--------|
| **CI/CD Build Time** | 6-11 min | 1-2 min | ‚è±Ô∏è 75-85% faster |
| **Deploy Frequency** | Slow (discouraged) | Fast (encouraged) | üìà More releases |
| **Developer Experience** | Frustrating waits | Instant feedback | üòä Happier team |
| **Infrastructure Cost** | Higher (long builds) | Lower (short builds) | üí∞ Save $$ |
| **Response Time** | 50-100ms | 15-30ms | ‚ö° Better UX |

---

## üö¶ Decision Matrix

**Choose ONNX Runtime if:**

- ‚úÖ You want 75-85% faster builds
- ‚úÖ You can invest 1-2 days for migration
- ‚úÖ You want same accuracy with better performance
- ‚úÖ You care about long-term maintainability

**Choose Cached Docker if:**

- ‚úÖ You need immediate relief (today)
- ‚úÖ You can't change code right now
- ‚úÖ You have CI/CD pipeline with caching

**Choose Rule-Based Only if:**

- ‚ö†Ô∏è Accuracy <70% is acceptable (NOT RECOMMENDED for mental health)

---

## üìû Next Steps

1. **Review this document** with your team
2. **Test short-term fix** (cached Docker) today
3. **Schedule ONNX migration** for next sprint
4. **Monitor metrics** after each phase
5. **Iterate** based on results

**Questions?** Check:

- `/docs/LOCAL_LLM_SPECIFICATIONS.md` - Current implementation details
- `/scripts/test_sta_ml.py` - Accuracy testing
- HuggingFace Optimum docs: <https://huggingface.co/docs/optimum>

---

**Document Version:** 1.0  
**Last Updated:** October 25, 2025  
**Author:** UGM-AICare Development Team
