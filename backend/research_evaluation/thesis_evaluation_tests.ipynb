{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f153edc",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddf475d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import asyncio\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add backend to path\n",
    "backend_path = Path.cwd().parent if 'research_evaluation' in str(Path.cwd()) else Path.cwd()\n",
    "sys.path.insert(0, str(backend_path))\n",
    "\n",
    "print(f\"‚úÖ Backend path: {backend_path}\")\n",
    "print(f\"‚úÖ Working directory: {Path.cwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aced7ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import app modules\n",
    "from app.database import AsyncSessionLocal\n",
    "from app.agents.sta.gemini_classifier import GeminiSTAClassifier\n",
    "from app.agents.ia.service import InsightsAgentService\n",
    "from app.agents.ia.schemas import IAQueryRequest, IAQueryParams\n",
    "\n",
    "# Alias for easier usage\n",
    "async_session_maker = AsyncSessionLocal\n",
    "\n",
    "print(\"‚úÖ App modules imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c802903",
   "metadata": {},
   "source": [
    "---\n",
    "## RQ1: Crisis Detection Accuracy (STA)\n",
    "\n",
    "**Hypothesis**: STA can accurately classify crisis vs non-crisis messages with ‚â•90% accuracy.\n",
    "\n",
    "**Method**: Test 50 synthetic scenarios (25 crisis, 25 non-crisis) and calculate:\n",
    "- Sensitivity (True Positive Rate)\n",
    "- Specificity (True Negative Rate)\n",
    "- Accuracy\n",
    "- Precision\n",
    "- F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd36dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load crisis scenarios from RQ1\n",
    "import importlib.util\n",
    "spec = importlib.util.spec_from_file_location(\n",
    "    \"crisis_scenarios\", \n",
    "    Path.cwd() / 'rq1_crisis_detection' / 'crisis_scenarios.py'\n",
    ")\n",
    "if spec and spec.loader:\n",
    "    crisis_scenarios_module = importlib.util.module_from_spec(spec)\n",
    "    spec.loader.exec_module(crisis_scenarios_module)\n",
    "    \n",
    "    CRISIS_SCENARIOS = crisis_scenarios_module.CRISIS_SCENARIOS\n",
    "    NON_CRISIS_SCENARIOS = crisis_scenarios_module.NON_CRISIS_SCENARIOS\n",
    "    \n",
    "    print(f\"‚úÖ Loaded {len(CRISIS_SCENARIOS)} crisis scenarios\")\n",
    "    print(f\"‚úÖ Loaded {len(NON_CRISIS_SCENARIOS)} non-crisis scenarios\")\n",
    "    print(f\"üìä Total scenarios: {len(CRISIS_SCENARIOS) + len(NON_CRISIS_SCENARIOS)}\")\n",
    "else:\n",
    "    print(\"‚ùå Failed to load crisis_scenarios module\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76201ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview crisis scenarios\n",
    "crisis_df = pd.DataFrame([{\n",
    "    'ID': s.id,\n",
    "    'Category': s.category,\n",
    "    'Severity': s.severity_if_crisis,\n",
    "    'Language': s.language,\n",
    "    'Text Preview': s.text[:80] + '...'\n",
    "} for s in CRISIS_SCENARIOS[:5]])\n",
    "\n",
    "print(\"\\nüî¥ Crisis Scenarios (First 5):\")\n",
    "display(crisis_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df617051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview non-crisis scenarios\n",
    "non_crisis_df = pd.DataFrame([{\n",
    "    'ID': s.id,\n",
    "    'Category': s.category,\n",
    "    'Language': s.language,\n",
    "    'Text Preview': s.text[:80] + '...'\n",
    "} for s in NON_CRISIS_SCENARIOS[:5]])\n",
    "\n",
    "print(\"\\nüü¢ Non-Crisis Scenarios (First 5):\")\n",
    "display(non_crisis_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10808fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RQ1: Run STA evaluation\n",
    "async def evaluate_sta():\n",
    "    \"\"\"Evaluate STA crisis detection on all 50 scenarios.\"\"\"\n",
    "    \n",
    "    classifier = GeminiSTAClassifier()\n",
    "    results = []\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üß™ Running RQ1: STA Crisis Detection Evaluation\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Evaluate crisis scenarios\n",
    "    print(\"\\nüî¥ Testing Crisis Scenarios (n=25)...\")\n",
    "    for i, scenario in enumerate(CRISIS_SCENARIOS, 1):\n",
    "        print(f\"   [{i}/25] {scenario.id}...\", end=' ')\n",
    "        try:\n",
    "            # Create request payload with session_id\n",
    "            from app.agents.sta.schemas import STAClassifyRequest\n",
    "            request = STAClassifyRequest(\n",
    "                text=scenario.text,\n",
    "                session_id=f\"eval_{scenario.id}\"\n",
    "            )\n",
    "            \n",
    "            # Use classify method\n",
    "            assessment = await classifier.classify(request)\n",
    "            predicted = \"crisis\" if assessment.risk_level >= 2 else \"non-crisis\"\n",
    "            correct = predicted == scenario.true_label\n",
    "            \n",
    "            results.append({\n",
    "                'id': scenario.id,\n",
    "                'true_label': scenario.true_label,\n",
    "                'predicted_label': predicted,\n",
    "                'risk_level': assessment.risk_level,\n",
    "                'correct': correct,\n",
    "                'category': scenario.category\n",
    "            })\n",
    "            \n",
    "            print(\"‚úÖ\" if correct else \"‚ùå\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error: {e}\")\n",
    "    \n",
    "    # Evaluate non-crisis scenarios\n",
    "    print(\"\\nüü¢ Testing Non-Crisis Scenarios (n=25)...\")\n",
    "    for i, scenario in enumerate(NON_CRISIS_SCENARIOS, 1):\n",
    "        print(f\"   [{i}/25] {scenario.id}...\", end=' ')\n",
    "        try:\n",
    "            # Create request payload with session_id\n",
    "            from app.agents.sta.schemas import STAClassifyRequest\n",
    "            request = STAClassifyRequest(\n",
    "                text=scenario.text,\n",
    "                session_id=f\"eval_{scenario.id}\"\n",
    "            )\n",
    "            \n",
    "            # Use classify method\n",
    "            assessment = await classifier.classify(request)\n",
    "            predicted = \"crisis\" if assessment.risk_level >= 2 else \"non-crisis\"\n",
    "            correct = predicted == scenario.true_label\n",
    "            \n",
    "            results.append({\n",
    "                'id': scenario.id,\n",
    "                'true_label': scenario.true_label,\n",
    "                'predicted_label': predicted,\n",
    "                'risk_level': assessment.risk_level,\n",
    "                'correct': correct,\n",
    "                'category': scenario.category\n",
    "            })\n",
    "            \n",
    "            print(\"‚úÖ\" if correct else \"‚ùå\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error: {e}\")\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Run evaluation\n",
    "rq1_results = await evaluate_sta()\n",
    "print(\"\\n‚úÖ RQ1 evaluation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d591173a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate RQ1 metrics\n",
    "def calculate_metrics(df):\n",
    "    \"\"\"Calculate confusion matrix and performance metrics.\"\"\"\n",
    "    \n",
    "    # Confusion matrix\n",
    "    tp = len(df[(df.true_label == 'crisis') & (df.predicted_label == 'crisis')])\n",
    "    tn = len(df[(df.true_label == 'non-crisis') & (df.predicted_label == 'non-crisis')])\n",
    "    fp = len(df[(df.true_label == 'non-crisis') & (df.predicted_label == 'crisis')])\n",
    "    fn = len(df[(df.true_label == 'crisis') & (df.predicted_label == 'non-crisis')])\n",
    "    \n",
    "    # Metrics\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    accuracy = (tp + tn) / len(df)\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    f1 = 2 * (precision * sensitivity) / (precision + sensitivity) if (precision + sensitivity) > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'confusion_matrix': {'TP': tp, 'TN': tn, 'FP': fp, 'FN': fn},\n",
    "        'sensitivity': sensitivity,\n",
    "        'specificity': specificity,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'f1_score': f1\n",
    "    }\n",
    "\n",
    "metrics = calculate_metrics(rq1_results)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä RQ1 Results: STA Crisis Detection Performance\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüéØ Confusion Matrix:\")\n",
    "print(f\"   True Positives (TP):  {metrics['confusion_matrix']['TP']}\")\n",
    "print(f\"   True Negatives (TN):  {metrics['confusion_matrix']['TN']}\")\n",
    "print(f\"   False Positives (FP): {metrics['confusion_matrix']['FP']}\")\n",
    "print(f\"   False Negatives (FN): {metrics['confusion_matrix']['FN']}\")\n",
    "\n",
    "print(\"\\nüìà Performance Metrics:\")\n",
    "print(f\"   Sensitivity (Recall): {metrics['sensitivity']*100:.2f}%\")\n",
    "print(f\"   Specificity:          {metrics['specificity']*100:.2f}%\")\n",
    "print(f\"   Accuracy:             {metrics['accuracy']*100:.2f}%\")\n",
    "print(f\"   Precision:            {metrics['precision']*100:.2f}%\")\n",
    "print(f\"   F1 Score:             {metrics['f1_score']:.4f}\")\n",
    "\n",
    "# Check if hypothesis is met\n",
    "hypothesis_met = metrics['accuracy'] >= 0.90\n",
    "print(f\"\\nüéì Hypothesis (Accuracy ‚â• 90%): {'‚úÖ MET' if hypothesis_met else '‚ùå NOT MET'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267579c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Confusion matrix heatmap\n",
    "plt.subplot(1, 2, 1)\n",
    "cm = np.array([\n",
    "    [metrics['confusion_matrix']['TP'], metrics['confusion_matrix']['FN']],\n",
    "    [metrics['confusion_matrix']['FP'], metrics['confusion_matrix']['TN']]\n",
    "])\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Predicted Crisis', 'Predicted Non-Crisis'],\n",
    "            yticklabels=['Actual Crisis', 'Actual Non-Crisis'])\n",
    "plt.title('Confusion Matrix')\n",
    "\n",
    "# Metrics bar chart\n",
    "plt.subplot(1, 2, 2)\n",
    "metric_names = ['Sensitivity', 'Specificity', 'Accuracy', 'Precision', 'F1 Score']\n",
    "metric_values = [\n",
    "    metrics['sensitivity']*100,\n",
    "    metrics['specificity']*100,\n",
    "    metrics['accuracy']*100,\n",
    "    metrics['precision']*100,\n",
    "    metrics['f1_score']*100\n",
    "]\n",
    "colors = ['#FF6B6B' if v < 90 else '#51CF66' for v in metric_values]\n",
    "plt.bar(metric_names, metric_values, color=colors, alpha=0.7)\n",
    "plt.axhline(y=90, color='r', linestyle='--', label='90% Threshold')\n",
    "plt.ylabel('Score (%)')\n",
    "plt.title('Performance Metrics')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Visualizations generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ee5467",
   "metadata": {},
   "source": [
    "---\n",
    "## RQ2: Orchestration Flow Correctness (Aika)\n",
    "\n",
    "**Hypothesis**: Aika meta-agent correctly orchestrates sub-agents (STA, SCA, SDA, IA) based on conversation context.\n",
    "\n",
    "**Method**: Test 10 representative conversation flows (F1-F10) and validate:\n",
    "- Correct agent sequence execution\n",
    "- Proper state transitions\n",
    "- Langfuse trace completeness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f235d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load orchestration flows from RQ2\n",
    "import importlib.util\n",
    "spec = importlib.util.spec_from_file_location(\n",
    "    \"orchestration_flows\",\n",
    "    Path.cwd() / 'rq2_orchestration' / 'orchestration_flows.py'\n",
    ")\n",
    "if spec and spec.loader:\n",
    "    orchestration_flows_module = importlib.util.module_from_spec(spec)\n",
    "    spec.loader.exec_module(orchestration_flows_module)\n",
    "    \n",
    "    ORCHESTRATION_FLOWS = orchestration_flows_module.ORCHESTRATION_FLOWS\n",
    "    \n",
    "    print(f\"‚úÖ Loaded {len(ORCHESTRATION_FLOWS)} orchestration flows\")\n",
    "    \n",
    "    # Preview flows\n",
    "    flows_df = pd.DataFrame([{\n",
    "        'ID': f.id,\n",
    "        'Name': f.name,\n",
    "        'Expected Sequence': ' ‚Üí '.join(f.expected_agent_sequence),\n",
    "        'Validation Criteria': len(f.validation_criteria)\n",
    "    } for f in ORCHESTRATION_FLOWS])\n",
    "    \n",
    "    print(\"\\nüîÑ Orchestration Flows:\")\n",
    "    display(flows_df)\n",
    "else:\n",
    "    print(\"‚ùå Failed to load orchestration_flows module\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226e978e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RQ2: Manual testing instructions\n",
    "print(\"=\"*80)\n",
    "print(\"üìù RQ2: Orchestration Flow Testing (Manual + Langfuse)\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\n‚ö†Ô∏è  This test requires manual execution via API and Langfuse trace validation.\\n\")\n",
    "print(\"Instructions:\")\n",
    "print(\"1. Start the backend server: cd backend && ./dev.sh\")\n",
    "print(\"2. Open Langfuse dashboard: http://localhost:3000\")\n",
    "print(\"3. For each flow (F1-F10), send the user_messages to /api/v1/aika\")\n",
    "print(\"4. Capture the Langfuse trace ID\")\n",
    "print(\"5. Validate against expected_agent_sequence\")\n",
    "print(\"6. Record results in the table below\\n\")\n",
    "\n",
    "# Create results template\n",
    "rq2_results_template = pd.DataFrame([{\n",
    "    'Flow ID': f.id,\n",
    "    'Name': f.name,\n",
    "    'Expected Sequence': ' ‚Üí '.join(f.expected_agent_sequence),\n",
    "    'Actual Sequence': '',  # Fill manually\n",
    "    'Langfuse Trace ID': '',  # Fill manually\n",
    "    'Match': False,  # Fill manually\n",
    "    'Notes': ''\n",
    "} for f in ORCHESTRATION_FLOWS])\n",
    "\n",
    "print(\"üìä Results Template (Fill after testing):\")\n",
    "display(rq2_results_template)\n",
    "\n",
    "print(\"\\nüí° Tip: Export results to CSV after filling: rq2_results.to_csv('rq2_results.csv', index=False)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b10e2c",
   "metadata": {},
   "source": [
    "---\n",
    "## RQ3: Coaching Quality Assessment (SCA)\n",
    "\n",
    "**Hypothesis**: SCA interventions meet quality standards for empathy, CBT techniques, and cultural appropriateness.\n",
    "\n",
    "**Method**: Dual-rater assessment (researcher + GPT-4) using 5-point Likert rubric:\n",
    "- Empathy & Validation (1-5)\n",
    "- CBT Technique Application (1-5)\n",
    "- Cultural Appropriateness (1-5)\n",
    "- Boundary Respect (1-5)\n",
    "- Resource Usefulness (1-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0af920c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load coaching scenarios from RQ3\n",
    "import importlib.util\n",
    "spec = importlib.util.spec_from_file_location(\n",
    "    \"coaching_scenarios\",\n",
    "    Path.cwd() / 'rq3_coaching_quality' / 'coaching_scenarios.py'\n",
    ")\n",
    "if spec and spec.loader:\n",
    "    coaching_scenarios_module = importlib.util.module_from_spec(spec)\n",
    "    spec.loader.exec_module(coaching_scenarios_module)\n",
    "    \n",
    "    COACHING_SCENARIOS = coaching_scenarios_module.COACHING_SCENARIOS\n",
    "    \n",
    "    print(f\"‚úÖ Loaded {len(COACHING_SCENARIOS)} coaching scenarios\")\n",
    "    \n",
    "    # Preview scenarios\n",
    "    coaching_df = pd.DataFrame([{\n",
    "        'ID': s.id,\n",
    "        'Category': s.category,\n",
    "        'Expected Intervention': s.expected_intervention_type,\n",
    "        'Focus': s.evaluation_focus,\n",
    "        'Message Preview': s.user_message[:80] + '...'\n",
    "    } for s in COACHING_SCENARIOS])\n",
    "    \n",
    "    print(\"\\nüéì Coaching Scenarios:\")\n",
    "    display(coaching_df)\n",
    "else:\n",
    "    print(\"‚ùå Failed to load coaching_scenarios module\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecce8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RQ3: Manual evaluation instructions\n",
    "print(\"=\"*80)\n",
    "print(\"üìù RQ3: Coaching Quality Assessment (Manual Dual-Rating)\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\n‚ö†Ô∏è  This test requires manual dual-rater assessment using the rubric.\\n\")\n",
    "print(\"Instructions:\")\n",
    "print(\"1. For each scenario, send user_message to SCA via /api/v1/agents/graph/sca/execute\")\n",
    "print(\"2. Review the intervention plan generated by SCA\")\n",
    "print(\"3. Rate on 5-point scale (1=Poor, 5=Excellent) for each dimension:\")\n",
    "print(\"   - Empathy & Validation\")\n",
    "print(\"   - CBT Technique Application\")\n",
    "print(\"   - Cultural Appropriateness\")\n",
    "print(\"   - Boundary Respect\")\n",
    "print(\"   - Resource Usefulness\")\n",
    "print(\"4. Repeat with GPT-4 as second rater\")\n",
    "print(\"5. Calculate inter-rater reliability (Cohen's Kappa)\\n\")\n",
    "\n",
    "# Create rubric template\n",
    "rq3_rubric_template = pd.DataFrame([{\n",
    "    'Scenario ID': s.id,\n",
    "    'Category': s.category,\n",
    "    'Researcher_Empathy': 0,\n",
    "    'Researcher_CBT': 0,\n",
    "    'Researcher_Culture': 0,\n",
    "    'Researcher_Boundary': 0,\n",
    "    'Researcher_Resources': 0,\n",
    "    'GPT4_Empathy': 0,\n",
    "    'GPT4_CBT': 0,\n",
    "    'GPT4_Culture': 0,\n",
    "    'GPT4_Boundary': 0,\n",
    "    'GPT4_Resources': 0,\n",
    "    'Notes': ''\n",
    "} for s in COACHING_SCENARIOS])\n",
    "\n",
    "print(\"üìä Rubric Template (Fill after testing):\")\n",
    "display(rq3_rubric_template)\n",
    "\n",
    "print(\"\\nüí° Tip: Calculate average scores and Cohen's Kappa after completing ratings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbd275e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper: Calculate Cohen's Kappa for inter-rater reliability\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "def calculate_inter_rater_reliability(df):\n",
    "    \"\"\"Calculate Cohen's Kappa for each rubric dimension.\"\"\"\n",
    "    \n",
    "    dimensions = ['Empathy', 'CBT', 'Culture', 'Boundary', 'Resources']\n",
    "    kappas = {}\n",
    "    \n",
    "    for dim in dimensions:\n",
    "        researcher_col = f'Researcher_{dim}'\n",
    "        gpt4_col = f'GPT4_{dim}'\n",
    "        \n",
    "        if researcher_col in df.columns and gpt4_col in df.columns:\n",
    "            researcher_scores = df[researcher_col].values\n",
    "            gpt4_scores = df[gpt4_col].values\n",
    "            \n",
    "            # Remove zeros (unfilled)\n",
    "            mask = (researcher_scores > 0) & (gpt4_scores > 0)\n",
    "            if mask.sum() > 0:\n",
    "                kappa = cohen_kappa_score(researcher_scores[mask], gpt4_scores[mask])\n",
    "                kappas[dim] = kappa\n",
    "    \n",
    "    return kappas\n",
    "\n",
    "print(\"‚úÖ Inter-rater reliability calculator ready\")\n",
    "print(\"\\nüí° Usage: kappas = calculate_inter_rater_reliability(rq3_results)\")\n",
    "print(\"   Interpretation:\")\n",
    "print(\"   - Œ∫ < 0.20: Slight agreement\")\n",
    "print(\"   - 0.21-0.40: Fair agreement\")\n",
    "print(\"   - 0.41-0.60: Moderate agreement\")\n",
    "print(\"   - 0.61-0.80: Substantial agreement\")\n",
    "print(\"   - 0.81-1.00: Almost perfect agreement\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855037a1",
   "metadata": {},
   "source": [
    "---\n",
    "## RQ4: Privacy Preservation (k-Anonymity)\n",
    "\n",
    "**Hypothesis**: Insights Agent (IA) enforces k-anonymity (k‚â•5) across all analytics queries.\n",
    "\n",
    "**Method**: Unit tests validating:\n",
    "1. Small cohort suppression (n<5 should be filtered)\n",
    "2. Compliant publication (n‚â•5 should pass)\n",
    "3. Individual query blocking (no individual-level data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6994a03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RQ4: k-Anonymity validation\n",
    "from app.agents.ia.queries import ALLOWED_QUERIES\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üîí RQ4: k-Anonymity Enforcement Validation\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüìã IA Analytics Queries (n=6):\")\n",
    "for i, (question_id, query) in enumerate(ALLOWED_QUERIES.items(), 1):\n",
    "    print(f\"   {i}. {question_id}\")\n",
    "\n",
    "print(\"\\nüîç Checking k-anonymity enforcement in SQL queries...\")\n",
    "\n",
    "k_anonymity_check = []\n",
    "for question_id, query in ALLOWED_QUERIES.items():\n",
    "    has_having_clause = 'HAVING COUNT(*)' in query.upper()\n",
    "    has_k5 = '>= 5' in query\n",
    "    \n",
    "    k_anonymity_check.append({\n",
    "        'Query': question_id,\n",
    "        'Has HAVING Clause': has_having_clause,\n",
    "        'Has k‚â•5 Constraint': has_k5,\n",
    "        'k-Anonymity Enforced': has_having_clause and has_k5\n",
    "    })\n",
    "\n",
    "k_check_df = pd.DataFrame(k_anonymity_check)\n",
    "display(k_check_df)\n",
    "\n",
    "all_enforced = k_check_df['k-Anonymity Enforced'].all()\n",
    "print(f\"\\n‚úÖ k-Anonymity Enforcement: {'PASS' if all_enforced else 'FAIL'}\")\n",
    "print(f\"   {k_check_df['k-Anonymity Enforced'].sum()}/{len(k_check_df)} queries enforce k‚â•5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3b6f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RQ4: Test small cohort suppression\n",
    "async def test_small_cohort_suppression():\n",
    "    \"\"\"Test that cohorts with n<5 users are suppressed.\"\"\"\n",
    "    \n",
    "    print(\"\\nüß™ Test 1: Small Cohort Suppression (n<5)\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    async with async_session_maker() as db:\n",
    "        ia_service = InsightsAgentService(db)\n",
    "        \n",
    "        # Test with crisis_trend query\n",
    "        # Construct IAQueryParams properly using model_validate\n",
    "        query_params = IAQueryParams.model_validate({\n",
    "            \"from\": datetime.now() - timedelta(days=7),\n",
    "            \"to\": datetime.now()\n",
    "        })\n",
    "        \n",
    "        request = IAQueryRequest(\n",
    "            question_id=\"crisis_trend\",\n",
    "            params=query_params\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            response = await ia_service.query(request)\n",
    "            \n",
    "            # Check if small cohorts are filtered\n",
    "            has_data = len(response.table) > 0\n",
    "            \n",
    "            if has_data:\n",
    "                # Verify all groups have ‚â•5 records\n",
    "                min_count = min(row.get('crisis_count', 999) for row in response.table)\n",
    "                print(f\"   üìä Query returned {len(response.table)} groups\")\n",
    "                print(f\"   üìè Minimum group size: {min_count}\")\n",
    "                print(f\"   ‚úÖ k-Anonymity: {'PASS' if min_count >= 5 else 'FAIL'}\")\n",
    "            else:\n",
    "                print(f\"   üìä Query returned 0 groups (all suppressed)\")\n",
    "                print(f\"   ‚úÖ k-Anonymity: PASS (no data below threshold)\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Error: {e}\")\n",
    "\n",
    "# Run test\n",
    "await test_small_cohort_suppression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d30392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RQ4: Test all IA queries for k-anonymity\n",
    "async def test_all_ia_queries():\n",
    "    \"\"\"Test all 6 IA queries for k-anonymity enforcement.\"\"\"\n",
    "    \n",
    "    print(\"\\nüß™ Test 2: All IA Queries k-Anonymity Check\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    async with async_session_maker() as db:\n",
    "        ia_service = InsightsAgentService(db)\n",
    "        results = []\n",
    "        \n",
    "        for question_id in ALLOWED_QUERIES.keys():\n",
    "            print(f\"\\n   Testing: {question_id}\")\n",
    "            \n",
    "            # Construct IAQueryParams properly using model_validate\n",
    "            query_params = IAQueryParams.model_validate({\n",
    "                \"from\": datetime.now() - timedelta(days=30),\n",
    "                \"to\": datetime.now()\n",
    "            })\n",
    "            \n",
    "            request = IAQueryRequest(\n",
    "                question_id=question_id,\n",
    "                params=query_params\n",
    "            )\n",
    "            \n",
    "            try:\n",
    "                response = await ia_service.query(request)\n",
    "                \n",
    "                has_data = len(response.table) > 0\n",
    "                k_anonymity_pass = True\n",
    "                \n",
    "                if has_data:\n",
    "                    # Check minimum count in any relevant column\n",
    "                    count_columns = [col for col in response.table[0].keys() if 'count' in col.lower()]\n",
    "                    if count_columns:\n",
    "                        min_count = min(\n",
    "                            min(row.get(col, 999) for col in count_columns)\n",
    "                            for row in response.table\n",
    "                        )\n",
    "                        k_anonymity_pass = min_count >= 5\n",
    "                        print(f\"      Groups: {len(response.table)}, Min count: {min_count}\")\n",
    "                    else:\n",
    "                        print(f\"      Groups: {len(response.table)}, No count column found\")\n",
    "                else:\n",
    "                    print(f\"      No data (all suppressed or no matches)\")\n",
    "                \n",
    "                results.append({\n",
    "                    'Query': question_id,\n",
    "                    'Has Data': has_data,\n",
    "                    'k-Anonymity': '‚úÖ PASS' if k_anonymity_pass else '‚ùå FAIL'\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"      ‚ùå Error: {e}\")\n",
    "                results.append({\n",
    "                    'Query': question_id,\n",
    "                    'Has Data': False,\n",
    "                    'k-Anonymity': '‚ö†Ô∏è ERROR'\n",
    "                })\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"üìä k-Anonymity Test Results\")\n",
    "        print(\"=\"*80)\n",
    "        display(pd.DataFrame(results))\n",
    "        \n",
    "        pass_count = sum(1 for r in results if 'PASS' in r['k-Anonymity'])\n",
    "        print(f\"\\n‚úÖ Overall: {pass_count}/{len(results)} queries enforce k-anonymity correctly\")\n",
    "\n",
    "# Run test\n",
    "await test_all_ia_queries()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95e8092",
   "metadata": {},
   "source": [
    "---\n",
    "## Export Results\n",
    "\n",
    "Export all test results for thesis documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871cb553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results directory\n",
    "results_dir = Path.cwd() / 'results'\n",
    "results_dir.mkdir(exist_ok=True)\n",
    "\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "print(\"üíæ Exporting results...\\n\")\n",
    "\n",
    "# Export RQ1 results\n",
    "if 'rq1_results' in locals():\n",
    "    rq1_path = results_dir / f'rq1_sta_results_{timestamp}.csv'\n",
    "    rq1_results.to_csv(rq1_path, index=False)\n",
    "    print(f\"‚úÖ RQ1 results exported: {rq1_path}\")\n",
    "    \n",
    "    # Export metrics\n",
    "    metrics_path = results_dir / f'rq1_metrics_{timestamp}.json'\n",
    "    with open(metrics_path, 'w') as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "    print(f\"‚úÖ RQ1 metrics exported: {metrics_path}\")\n",
    "\n",
    "# Export templates for manual tests\n",
    "rq2_template_path = results_dir / f'rq2_orchestration_template_{timestamp}.csv'\n",
    "rq2_results_template.to_csv(rq2_template_path, index=False)\n",
    "print(f\"‚úÖ RQ2 template exported: {rq2_template_path}\")\n",
    "\n",
    "rq3_template_path = results_dir / f'rq3_coaching_rubric_{timestamp}.csv'\n",
    "rq3_rubric_template.to_csv(rq3_template_path, index=False)\n",
    "print(f\"‚úÖ RQ3 template exported: {rq3_template_path}\")\n",
    "\n",
    "print(f\"\\nüìÅ All results saved to: {results_dir}\")\n",
    "print(\"\\nüéì Ready for thesis documentation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6c5153",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "This notebook consolidates all 4 research question evaluations:\n",
    "\n",
    "- ‚úÖ **RQ1**: STA crisis detection tested with 50 scenarios\n",
    "- ‚úÖ **RQ2**: Orchestration flows template ready for manual testing\n",
    "- ‚úÖ **RQ3**: Coaching quality rubric template ready for dual-rating\n",
    "- ‚úÖ **RQ4**: k-Anonymity enforcement validated across all IA queries\n",
    "\n",
    "**Next Steps**:\n",
    "1. Complete RQ2 manual testing with Langfuse traces\n",
    "2. Complete RQ3 dual-rater assessment\n",
    "3. Compile all results for thesis Chapter 4 (Results)\n",
    "4. Calculate final statistics and create visualizations"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
