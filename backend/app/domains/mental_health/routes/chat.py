"""Chat endpoints covering REST requests and streaming WebSocket support."""

from __future__ import annotations

import json
import logging
from datetime import datetime
from typing import Any, Callable, Dict, List, Optional, cast

from fastapi import (
    APIRouter,
    Body,
    Depends,
    HTTPException,
    Query,
    WebSocket,
    WebSocketDisconnect,
    status,
)
from sqlalchemy import select
from sqlalchemy.ext.asyncio import AsyncSession

from app import auth_utils
from app.agents.aika.tools import get_aika_tools
from app.agents.aika import AikaOrchestrator  # âœ¨ New: Meta-Agent Orchestrator
from app.core import llm
from app.core.rate_limiter import check_rate_limit_dependency, get_rate_limiter
from app.database import get_async_db
from app.dependencies import get_current_active_user
from app.models import User  # Core model
from app.domains.mental_health.models import Conversation, UserSummary
from app.domains.mental_health.schemas.chat import ChatRequest, ChatResponse, ConversationHistoryItem
# Note: chat_processing module needs to be created - using stub functions
from app.domains.mental_health.services.personal_context import (
    build_user_personal_context,
    invalidate_user_personal_context,
)
from app.domains.mental_health.services.tool_calling import generate_with_tools

# Temporary stub functions until chat_processing module is created
async def process_chat_event(*args, **kwargs):
    """Stub function - needs implementation in chat_processing.py"""
    raise NotImplementedError("process_chat_event not yet implemented")

async def process_chat_message(*args, **kwargs):
    """Stub function - needs implementation in chat_processing.py"""
    raise NotImplementedError("process_chat_message not yet implemented")

logger = logging.getLogger(__name__)

router = APIRouter(prefix="/api/v1", tags=["Chat"])

MIN_TURNS_FOR_SUMMARY = 1  # Reduced from 2 to 1 for better memory capture
MAX_HISTORY_CHARS_FOR_SUMMARY = 15000


def _compose_system_prompt(base_prompt: Optional[str], personal_context: str) -> Optional[str]:
    base = (base_prompt or "").strip()
    context = (personal_context or "").strip()
    if base and context:
        return f"{base}\n\n{context}"
    if context:
        return context
    if base:
        return base
    return None


async def _default_llm_responder(
    history: List[Dict[str, str]],
    system_prompt: Optional[str],
    request: ChatRequest,
    _stream_callback: Optional[Callable[[str], Any]] = None,
) -> str:
    """Default LLM responder without tool calling."""
    model_name = request.model or "gemini_google"
    return await llm.generate_response(
        history=history,
        model=model_name,
        max_tokens=request.max_tokens,
        temperature=request.temperature,
        system_prompt=system_prompt,
    )


async def _tool_aware_llm_responder(
    history: List[Dict[str, str]],
    system_prompt: Optional[str],
    request: ChatRequest,
    db: AsyncSession,
    user_id: int,
    stream_callback: Optional[Callable[[str], Any]] = None,
) -> str:
    """LLM responder with tool calling capabilities.
    
    This responder enables Aika to actively query the database for:
    - Conversation summaries from previous sessions
    - Journal entries with optional keyword search
    - Detailed conversation context from specific sessions
    
    Args:
        history: Conversation history
        system_prompt: System instruction
        request: Chat request configuration
        db: Database session for tool execution
        user_id: User ID for scoping tool queries
        stream_callback: Optional callback for streaming
        
    Returns:
        Generated response text
    """
    # Use the proper tool calling service
    from app.domains.mental_health.services.tool_calling import generate_with_tools
    
    response_text, tool_calls = await generate_with_tools(
        history=history,
        system_prompt=system_prompt,
        request=request,
        db=db,
        user_id=user_id,
        stream_callback=stream_callback,
    )
    
    if tool_calls:
        logger.info(f"Tool calling completed with {len(tool_calls)} tool call(s)")
    
    return response_text


async def _streaming_llm_responder(
    history: List[Dict[str, str]],
    system_prompt: Optional[str],
    request: ChatRequest,
    stream_callback: Optional[Callable[[str], Any]] = None,
) -> str:
    """Streaming LLM responder without tool calling."""
    if stream_callback is None:
        # Fallback to default behaviour if no callback is provided.
        return await _default_llm_responder(history, system_prompt, request, None)

    requested_model = request.model or "gemini_google"
    if requested_model == "gemini_google":
        model_name = getattr(llm, "DEFAULT_GEMINI_MODEL", "gemini-2.0-flash")
    else:
        model_name = requested_model
    full_text = ""
    async for chunk in llm.stream_gemini_response(
        history=history,
        model=model_name,
        max_tokens=request.max_tokens,
        temperature=request.temperature,
        system_prompt=system_prompt,
    ):
        if not chunk:
            continue
        full_text += chunk
        try:
            await stream_callback(chunk)
        except Exception as callback_err:  # pragma: no cover - defensive logging
            logger.warning("Streaming callback failed: %s", callback_err)
    return full_text


async def _streaming_tool_aware_llm_responder(
    history: List[Dict[str, str]],
    system_prompt: Optional[str],
    request: ChatRequest,
    db: AsyncSession,
    user_id: int,
    stream_callback: Optional[Callable[[str], Any]] = None,
) -> str:
    """Streaming LLM responder with tool calling capabilities.
    
    Note: Tool calling in streaming mode is complex. This implementation
    enables tools but may need enhancements for full tool calling loop support.
    
    Args:
        history: Conversation history
        system_prompt: System instruction
        request: Chat request configuration
        db: Database session for tool execution
        user_id: User ID for scoping tool queries
        stream_callback: Callback for streaming chunks
        
    Returns:
        Complete response text
    """
    if stream_callback is None:
        # Fallback to non-streaming tool-aware responder
        return await _tool_aware_llm_responder(history, system_prompt, request, db, user_id, None)

    # Use the proper tool calling service with streaming
    from app.domains.mental_health.services.tool_calling import generate_with_tools
    
    response_text, tool_calls = await generate_with_tools(
        history=history,
        system_prompt=system_prompt,
        request=request,
        db=db,
        user_id=user_id,
        stream_callback=stream_callback,
    )
    
    if tool_calls:
        logger.info(f"Streaming with tools completed, {len(tool_calls)} tool call(s)")
    
    return response_text


# ===================== NEW: AIKA META-AGENT ENDPOINT =====================

@router.post("/aika", dependencies=[Depends(check_rate_limit_dependency)])
async def handle_aika_request(
    request: ChatRequest = Body(...),
    current_user: User = Depends(get_current_active_user),
    db: AsyncSession = Depends(get_async_db),
) -> Dict:
    """
    ðŸŒŸ Aika Meta-Agent Endpoint
    
    This endpoint routes requests through Aika's orchestration graph,
    which intelligently coordinates STA, SCA, SDA, and IA based on user role.
    
    **User Roles:**
    - `user` (students): Safety triage â†’ Coaching â†’ Escalation if needed
    - `admin`: Analytics and administrative actions
    - `counselor`: Case management and clinical insights
    
    **Response Format:**
    ```json
    {
        "success": true,
        "response": "Aika's response",
        "metadata": {
            "session_id": "...",
            "agents_invoked": ["STA", "SCA"],
            "risk_level": "low",
            "processing_time_ms": 1234.56
        }
    }
    ```
    
    Args:
        request: Chat request with message and history
        current_user: Authenticated user
        db: Database session
        
    Returns:
        Dict with Aika's response and metadata
    """
    
    try:
        # Determine user role for routing
        # In production, this should check actual role from database
        user_role = "user"  # Default to student
        
        if hasattr(current_user, 'role'):
            if current_user.role == "admin":
                user_role = "admin"
            elif current_user.role == "counselor":
                user_role = "counselor"
        
        # Convert history from schema format to dict format
        conversation_history = []
        if request.history:
            for item in request.history:
                if isinstance(item, dict):
                    conversation_history.append(item)
                else:
                    # Pydantic model
                    conversation_history.append({
                        "role": item.role,
                        "content": item.content,
                    })
        
        # Initialize Aika orchestrator
        aika = AikaOrchestrator(db=db)
        
        # Process message through orchestration graph
        logger.info(
            f"âœ¨ Aika processing request from {user_role} user {current_user.id}: "
            f"{request.message[:50]}..."
        )
        
        result = await aika.process_message(
            user_id=current_user.id,
            user_role=user_role,
            message=request.message,
            session_id=request.session_id,
            conversation_history=conversation_history,
        )
        
        logger.info(
            f"âœ… Aika completed: agents={result['metadata']['agents_invoked']}, "
            f"time={result['metadata']['processing_time_ms']:.2f}ms"
        )
        
        return result
        
    except ValueError as exc:
        logger.warning(f"Value error in /aika endpoint: {exc}", exc_info=True)
        raise HTTPException(status_code=400, detail=str(exc)) from exc
    except HTTPException:
        raise
    except Exception as exc:
        logger.error(
            f"Unhandled exception in /aika endpoint for user {current_user.id}: {exc}",
            exc_info=True,
        )
        raise HTTPException(
            status_code=500,
            detail=f"Internal server error: {str(exc)}"
        ) from exc


# ===================== EXISTING LEGACY CHAT ENDPOINT =====================

@router.post("/chat", response_model=ChatResponse, dependencies=[Depends(check_rate_limit_dependency)])
async def handle_chat_request(
    request: ChatRequest = Body(...),
    current_user: User = Depends(get_current_active_user),
    db: AsyncSession = Depends(get_async_db),
    enable_tools: bool = Query(default=True, description="Enable tool calling for memory access"),
) -> ChatResponse:
    """Handle traditional REST chat requests.
    
    Args:
        request: Chat request with message, history, and configuration
        current_user: Authenticated user
        db: Database session
        enable_tools: Whether to enable tool calling (default: True)
        
    Returns:
        ChatResponse with assistant's reply and updated history
    """

    session_id = request.session_id
    conversation_id = request.conversation_id

    try:
        personal_context = await build_user_personal_context(db, current_user)
        active_system_prompt = _compose_system_prompt(request.system_prompt, personal_context)
        
        # DEBUG: Log system prompt to verify it's being received
        logger.info(f"ðŸŽ­ System prompt received from frontend: {request.system_prompt[:100] if request.system_prompt else 'None'}...")
        logger.info(f"ðŸ‘¤ Personal context: {personal_context[:100] if personal_context else 'None'}...")
        logger.info(f"âœ… Active system prompt (composed): {active_system_prompt[:100] if active_system_prompt else 'None'}...")

        # Choose LLM responder based on tool enablement
        if enable_tools:
            # Create a closure that includes db and user_id for tool execution
            async def tool_responder(hist, sysprompt, req, callback):
                return await _tool_aware_llm_responder(
                    history=hist,
                    system_prompt=sysprompt,
                    request=req,
                    db=db,
                    user_id=current_user.id,
                    stream_callback=callback,
                )
            llm_responder = tool_responder
            logger.info(f"Tool calling enabled for user {current_user.id}, session {session_id}")
        else:
            llm_responder = _default_llm_responder
            logger.debug(f"Tool calling disabled for user {current_user.id}, session {session_id}")

        if request.event:
            result = await process_chat_event(
                request=request,
                current_user=current_user,
                db=db,
                session_id=session_id,
                conversation_id=conversation_id,
                active_system_prompt=active_system_prompt,
            )
        elif request.message:
            result = await process_chat_message(
                request=request,
                current_user=current_user,
                db=db,
                session_id=session_id,
                conversation_id=conversation_id,
                active_system_prompt=active_system_prompt,
                schedule_summary=None,
                summarize_now=summarize_and_save,
                llm_responder=llm_responder,
            )
        else:
            raise HTTPException(status_code=400, detail="Invalid request: Missing message or event payload")

        return ChatResponse(
            response=result.response_text,
            provider_used=result.provider_used,
            model_used=result.model_used,
            history=result.final_history,
            module_completed_id=result.module_completed_id,
            intervention_plan=result.intervention_plan,
        )

    except ValueError as exc:
        logger.warning("Value error in /chat endpoint: %s", exc, exc_info=True)
        raise HTTPException(status_code=400, detail=str(exc)) from exc
    except HTTPException:
        raise
    except Exception as exc:  # pragma: no cover - defensive logging
        user_id_for_log = current_user.id if current_user else "unknown_user"
        logger.error(
            "Unhandled exception in /chat endpoint for user %s, session %s: %s",
            user_id_for_log,
            session_id,
            exc,
            exc_info=True,
        )
        raise HTTPException(status_code=500, detail="An internal server error occurred.") from exc


@router.websocket("/chat/ws")
async def chat_ws(
    websocket: WebSocket,
    token: str,
    db: AsyncSession = Depends(get_async_db),
) -> None:
    """Stream chat responses to the frontend via WebSocket."""
    try:
        payload = auth_utils.decrypt_and_validate_token(token)
    except HTTPException:
        await websocket.close(code=status.WS_1008_POLICY_VIOLATION)
        return

    user_id_str = payload.sub
    if user_id_str is None:
        await websocket.close(code=status.WS_1008_POLICY_VIOLATION)
        return

    try:
        user_id = int(user_id_str)
    except ValueError:
        await websocket.close(code=status.WS_1008_POLICY_VIOLATION)
        return

    user = await db.get(User, user_id)
    if not user:
        await websocket.close(code=status.WS_1008_POLICY_VIOLATION)
        return

    await websocket.accept()

    async def emit_token(chunk: str, *, session_id: str, conversation_id: str) -> None:
        await websocket.send_json(
            {
                "type": "token",
                "token": chunk,
                "sessionId": session_id,
                "conversationId": conversation_id,
            }
        )

    try:
        while True:
            message = await websocket.receive_text()
            try:
                raw_payload = json.loads(message)
            except json.JSONDecodeError as exc:
                await websocket.send_json({"type": "error", "detail": f"Invalid JSON: {exc}"})
                continue

            try:
                chat_request = ChatRequest.model_validate(raw_payload)
            except Exception as exc:  # pragma: no cover - defensive logging
                await websocket.send_json({"type": "error", "detail": f"Invalid payload: {exc}"})
                continue

            session_id = chat_request.session_id
            conversation_id = chat_request.conversation_id

            # Rate limiting check for WebSocket
            rate_limiter = get_rate_limiter()
            role = "admin" if user.is_admin else "student"
            is_allowed, remaining, reset_timestamp = await rate_limiter.check_rate_limit(
                user_id=user.id,
                endpoint="chat",
                role=role
            )
            
            if not is_allowed:
                # Send rate limit error to client
                retry_after = max(1, reset_timestamp - int(datetime.now().timestamp()))
                await websocket.send_json({
                    "type": "error",
                    "detail": {
                        "error": "Rate limit exceeded",
                        "message": f"Too many requests. Please try again in {retry_after} seconds.",
                        "retry_after": retry_after,
                        "reset_at": reset_timestamp,
                    }
                })
                continue

            # DEBUG: Log system prompt flow
            logger.info(f"ðŸŽ­ [WebSocket] System prompt from frontend: {chat_request.system_prompt[:100] if chat_request.system_prompt else 'None'}...")
            
            personal_context = await build_user_personal_context(db, user)
            logger.info(f"ðŸ‘¤ [WebSocket] Personal context: {personal_context[:100] if personal_context else 'None'}...")
            
            active_system_prompt = _compose_system_prompt(chat_request.system_prompt, personal_context)
            logger.info(f"âœ… [WebSocket] Active system prompt (composed): {active_system_prompt[:100] if active_system_prompt else 'None'}...")

            if chat_request.event:
                result = await process_chat_event(
                    request=chat_request,
                    current_user=user,
                    db=db,
                    session_id=session_id,
                    conversation_id=conversation_id,
                    active_system_prompt=active_system_prompt,
                )
                await websocket.send_json(
                    {
                        "type": "completed",
                        "response": result.response_text,
                        "history": result.final_history,
                        "provider": result.provider_used,
                        "model": result.model_used,
                        "moduleCompletedId": result.module_completed_id,
                        "sessionId": session_id,
                        "conversationId": conversation_id,
                    }
                )
                continue

            if not chat_request.message:
                await websocket.send_json({"type": "error", "detail": "Missing message payload."})
                continue

            async def stream_callback(chunk: str) -> None:
                await emit_token(chunk, session_id=session_id, conversation_id=conversation_id)

            # Use tool-aware streaming responder by default
            async def tool_aware_streaming_responder(hist, sysprompt, req, callback):
                return await _streaming_tool_aware_llm_responder(
                    history=hist,
                    system_prompt=sysprompt,
                    request=req,
                    db=db,
                    user_id=user.id,
                    stream_callback=callback,
                )

            try:
                result = await process_chat_message(
                    request=chat_request,
                    current_user=user,
                    db=db,
                    session_id=session_id,
                    conversation_id=conversation_id,
                    active_system_prompt=active_system_prompt,
                    schedule_summary=None,
                    summarize_now=summarize_and_save,
                    llm_responder=tool_aware_streaming_responder,
                    stream_callback=stream_callback,
                )
            except HTTPException as exc:
                await websocket.send_json({"type": "error", "detail": exc.detail})
                continue
            except Exception as exc:  # pragma: no cover - defensive logging
                logger.error("Streaming chat failed: %s", exc, exc_info=True)
                await websocket.send_json({"type": "error", "detail": "Internal server error."})
                continue

            await websocket.send_json(
                {
                    "type": "completed",
                    "response": result.response_text,
                    "history": result.final_history,
                    "provider": result.provider_used,
                    "model": result.model_used,
                    "moduleCompletedId": result.module_completed_id,
                    "interventionPlan": result.intervention_plan.model_dump() if result.intervention_plan else None,
                    "sessionId": session_id,
                    "conversationId": conversation_id,
                }
            )

    except WebSocketDisconnect:
        logger.debug("Chat WebSocket disconnected for user_id=%s", user_id)
    except Exception as exc:  # pragma: no cover - defensive logging
        logger.error("Chat WebSocket error for user_id=%s: %s", user_id, exc, exc_info=True)
        await websocket.close(code=status.WS_1011_INTERNAL_ERROR)


async def summarize_and_save(user_id: int, session_id_to_summarize: str) -> None:
    """Fetch history, create a summary, and persist it for future context injection."""
    logger.info(
        "Background Task: Starting summarization for user %s, session %s",
        user_id,
        session_id_to_summarize,
    )
    from app.database import AsyncSessionLocal

    async with AsyncSessionLocal() as db:
        try:
            stmt = (
                select(Conversation)
                .where(
                    Conversation.session_id == session_id_to_summarize,
                    Conversation.user_id == user_id,
                )
                .order_by(Conversation.timestamp.asc())
            )
            result = await db.execute(stmt)
            conversation_history = result.scalars().all()

            if not conversation_history or len(conversation_history) < MIN_TURNS_FOR_SUMMARY:
                logger.info(
                    "Background Task: Skipping summarization for session %s (too short).",
                    session_id_to_summarize,
                )
                return

            history_lines = []
            for turn in conversation_history:
                history_lines.append(f"user: {turn.message}")
                history_lines.append(f"assistant: {turn.response}")
            formatted_history = "\n".join(history_lines)

            if len(formatted_history) > MAX_HISTORY_CHARS_FOR_SUMMARY:
                original_len = len(formatted_history)
                formatted_history = formatted_history[-MAX_HISTORY_CHARS_FOR_SUMMARY :]
                logger.warning(
                    "Background Task: Truncated conversation history for session %s from %s to %s chars.",
                    session_id_to_summarize,
                    original_len,
                    len(formatted_history),
                )

            summarization_prompt = f"""Kamu adalah Aika, AI pendamping dari UGM-AICare. Tugasmu adalah membuat ringkasan singkat dari percakapan sebelumnya dengan pengguna. Ringkasan ini akan kamu gunakan untuk mengingatkan pengguna tentang apa yang telah dibahas jika mereka bertanya \"apakah kamu ingat percakapan kita?\".

Buatlah ringkasan dalam 1-2 kalimat saja, dalam Bahasa Indonesia yang alami dan kasual, seolah-olah kamu sedang berbicara santai dengan teman. Fokus pada inti atau perasaan utama yang diungkapkan pengguna.
Hindari penggunaan daftar, poin-poin, judul seperti \"Poin Utama\", atau format markdown. Cukup tuliskan sebagai paragraf singkat yang mengalir.

Contoh output yang baik:
\"kita sempat ngobrolin soal kamu yang lagi ngerasa nggak nyaman karena pernah gagal memimpin organisasi.\"
\"kamu cerita tentang perasaanmu yang campur aduk setelah kejadian di kampus.\"
\"kita kemarin membahas tentang kesulitanmu mencari teman dan bagaimana itu membuatmu merasa kesepian.\"

Percakapan yang perlu diringkas:
{formatted_history}

Ringkasan singkat dan kasual:"""

            summary_llm_history = [{"role": "user", "content": summarization_prompt}]

            summary_text = await llm.generate_response(
                history=summary_llm_history,
                model="gemini_google",
                max_tokens=2048,
                temperature=0.5,
            )

            if summary_text.startswith("Error:"):
                logger.error(
                    "Background Task: LLM error during summarization for session %s: %s",
                    session_id_to_summarize,
                    summary_text,
                )
                raise RuntimeError("LLM error during summarization")

            new_summary = UserSummary(
                user_id=user_id,
                summarized_session_id=session_id_to_summarize,
                summary_text=summary_text.strip(),
                timestamp=datetime.now(),
            )
            db.add(new_summary)
            await db.commit()
            logger.info(
                "Background Task: Saved summary for user %s from session %s",
                user_id,
                session_id_to_summarize,
            )
            await invalidate_user_personal_context(user_id)

        except Exception as exc:  # pragma: no cover - defensive logging
            await db.rollback()
            logger.error(
                "Background Task: Failed to summarize session %s for user %s: %s",
                session_id_to_summarize,
                user_id,
                exc,
                exc_info=True,
            )


@router.get("/history", response_model=List[ConversationHistoryItem])
async def get_chat_history(
    limit: int = Query(100, ge=1, le=500),
    skip: int = Query(0, ge=0),
    db: AsyncSession = Depends(get_async_db),
    current_user: User = Depends(get_current_active_user),
) -> List[ConversationHistoryItem]:
    """Return conversation history for the authenticated user."""
    try:
        stmt = (
            select(Conversation)
            .where(Conversation.user_id == current_user.id)
            .order_by(Conversation.timestamp.asc())
            .offset(skip)
            .limit(limit)
        )
        result = await db.execute(stmt)
        conversation_turns = result.scalars().all()

        history_items: List[Dict[str, Any]] = []
        for turn in conversation_turns:
            history_items.append(
                {
                    "role": "user",
                    "content": turn.message,
                    "timestamp": turn.timestamp,
                    "session_id": turn.session_id,
                }
            )
            history_items.append(
                {
                    "role": "assistant",
                    "content": turn.response,
                    "timestamp": turn.timestamp,
                    "session_id": turn.session_id,
                }
            )

        history_items.sort(key=lambda item: item["timestamp"], reverse=False)
        paginated_history = history_items[skip : skip + limit]
        return [ConversationHistoryItem(**item) for item in paginated_history]

    except Exception as exc:  # pragma: no cover - defensive logging
        logger.error(
            "Error fetching chat history for user %s: %s",
            current_user.id,
            exc,
            exc_info=True,
        )
        raise HTTPException(status_code=500, detail="Failed to retrieve chat history") from exc
