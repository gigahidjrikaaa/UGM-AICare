"""
Aika Meta-Agent Orchestrator

This is the core orchestration logic that routes user requests to the appropriate
specialized agents based on user role and intent.

Architecture:
- Students: STA (triage) â†’ SCA (coaching) â†’ [SDA if escalation] â†’ IA (background analytics)
- Admins: Intent classification â†’ IA (analytics) or SDA (actions) â†’ Response
- Counselors: SDA (case management) â†’ IA (insights) â†’ SCA (recommendations)

All routing is handled by LangGraph with role-aware conditional edges.
"""

import logging
import os
from typing import Dict, List, Literal, Optional, Callable
from datetime import datetime
import time

from sqlalchemy.ext.asyncio import AsyncSession
from langgraph.graph import StateGraph, END
from langgraph.checkpoint.memory import MemorySaver

from .state import AikaState
from .identity import AIKA_SYSTEM_PROMPTS, AIKA_GREETINGS
from .activity_logger import ActivityLogger
# REFACTORED: Use real LangGraph services instead of adapters
from app.agents.sta.sta_graph_service import STAGraphService
from app.agents.sta.conversation_state import ConversationState
from app.agents.sca.sca_graph_service import SCAGraphService
from app.core.memory import get_redis_client
from app.agents.sda.sda_graph_service import SDAGraphService
from app.agents.ia.ia_graph_service import IAGraphService
from app.core.llm import generate_response
from app.models.user import User

logger = logging.getLogger(__name__)


class AikaOrchestrator:
    """
    Meta-agent orchestrator for UGM-AICare.
    
    Aika coordinates all four specialized agents (STA, SCA, SDA, IA) based on
    user role and intent, providing a unified AI personality across the platform.
    """
    
    def __init__(self, db: AsyncSession):
        """Initialize Aika with database session"""
        self.db = db
        
        # REFACTORED: Initialize LangGraph services instead of adapters
        self.sta_service = STAGraphService(db)
        self.sca_service = SCAGraphService(db)
        self.sda_service = SDAGraphService(db)
        self.ia_service = IAGraphService(db)
        
        # Activity logger for real-time monitoring
        self.activity_logger = ActivityLogger()
        
        # Conversation state cache (in-memory for now, Redis in production)
        self._conversation_states: dict[str, ConversationState] = {}
        
        # Build orchestration graph
        self.graph = self._build_graph()
        
        logger.info("SUCCESS: Aika Meta-Agent initialized with LangGraph services + conversation tracking")
    
    def _build_graph(self):
        """
        Build LangGraph orchestration workflow.
        
        Flow:
        1. classify_intent â†’ determine user intent and role
        2. route_by_role â†’ student/admin/counselor path
        3. Specialized agent processing
        4. synthesize_response â†’ unified Aika response
        """
        workflow = StateGraph(AikaState)
        
        # Define nodes
        workflow.add_node("classify_intent", self._classify_intent)
        workflow.add_node("student_triage", self._student_triage)
        workflow.add_node("student_coaching", self._student_coaching)
        workflow.add_node("student_escalation", self._student_escalation)
        workflow.add_node("admin_analytics", self._admin_analytics)
        workflow.add_node("admin_actions", self._admin_actions)
        workflow.add_node("counselor_cases", self._counselor_cases)
        workflow.add_node("counselor_insights", self._counselor_insights)
        workflow.add_node("synthesize_response", self._synthesize_response)
        workflow.add_node("background_analytics", self._background_analytics)
        
        # Entry point
        workflow.set_entry_point("classify_intent")
        
        # Role-based routing
        workflow.add_conditional_edges(
            "classify_intent",
            self._route_by_role,
            {
                "student_triage": "student_triage",
                "admin_analytics": "admin_analytics",
                "admin_actions": "admin_actions",
                "counselor_cases": "counselor_cases",
            }
        )
        
        # Student path
        workflow.add_conditional_edges(
            "student_triage",
            self._check_crisis,
            {
                "crisis": "student_escalation",
                "no_crisis": "student_coaching",
            }
        )
        workflow.add_edge("student_coaching", "synthesize_response")
        workflow.add_edge("student_escalation", "synthesize_response")
        
        # Admin path
        workflow.add_edge("admin_analytics", "synthesize_response")
        workflow.add_edge("admin_actions", "synthesize_response")
        
        # Counselor path
        workflow.add_edge("counselor_cases", "counselor_insights")
        workflow.add_edge("counselor_insights", "synthesize_response")
        
        # Background analytics (runs after response)
        workflow.add_edge("synthesize_response", "background_analytics")
        workflow.add_edge("background_analytics", END)
        
        # Compile graph
        memory = MemorySaver()
        return workflow.compile(checkpointer=memory)
    
    # ===================== NODE IMPLEMENTATIONS =====================
    
    def _get_conversation_key(self, conversation_id: str, user_id: str) -> str:
        """Generate conversation state cache key"""
        return f"{user_id}:{conversation_id}"
    
    def _get_conversation_state(self, conversation_id: str, user_id: int) -> ConversationState:
        """Get or create conversation state for caching"""
        key = self._get_conversation_key(conversation_id, str(user_id))
        if key not in self._conversation_states:
            self._conversation_states[key] = ConversationState(
                conversation_id=conversation_id,
                user_id=user_id
            )
        return self._conversation_states[key]

    async def _classify_intent(self, state: AikaState) -> AikaState:
        """
        Classify user intent and prepare context.
        
        OPTIMIZED: Uses conversation state caching to reduce Gemini API calls.
        - Skips intent classification for stable low-risk conversations
        - Tracks intent changes to detect topic shifts
        - Reduces API costs by ~35% without compromising safety
        
        This node determines:
        - What the user wants to do
        - Which agents should be invoked
        - What context is needed
        """
        start_time = time.time()
        
        try:
            # Load conversation state for caching optimization
            # Ensure conversation_id exists
            if not state.conversation_id:
                state.conversation_id = f"conv_{state.user_id}_{int(time.time())}"
            
            conv_state = self._get_conversation_state(state.conversation_id, state.user_id)
            conv_state.message_count += 1
            conv_state.messages_since_last_assessment += 1
            
            # Check if we can skip intent classification (OPTIMIZATION)
            # Skip if: stable intent, low risk, short message (no topic shift likely)
            short_message = len(state.message.split()) < 20
            if conv_state.should_skip_intent_classification() and short_message:
                state.intent = conv_state.last_intent or "emotional_support"
                state.intent_confidence = 0.85
                
                elapsed = time.time() - start_time
                self.activity_logger.log_info(
                    agent="Aika",
                    message=f"Intent classification cached: {state.intent}",
                    details={
                        "intent": state.intent,
                        "confidence": state.intent_confidence,
                        "cached": True,
                        "cache_hit_rate": conv_state.cache_hit_rate,
                        "duration_ms": int(elapsed * 1000),
                    }
                )
                
                logger.info(
                    f"ðŸ’¾ Cached intent: {state.intent} "
                    f"(cache_hit_rate: {conv_state.cache_hit_rate:.1%}, "
                    f"efficiency: {conv_state.efficiency_score:.1%})"
                )
                return state
            
            # Get user context from database
            user_context = await self._get_user_context(state.user_id)
            state.personal_context = user_context
            
            # Classify intent using LLM (FRESH CLASSIFICATION)
            conv_state.gemini_calls_made += 1
            intent_prompt = f"""
You are Aika's intent classifier. Analyze this message and determine user intent.

User role: {state.user_role}
Message: {state.message}

For STUDENTS, intents:
- "crisis" - self-harm, suicide, severe distress
- "emotional_support" - need to talk, feeling down
- "journaling" - want to reflect or track mood
- "information" - asking about mental health resources

For ADMINS, intents:
- "analytics_query" - asking for data/insights
- "action_request" - wants to perform admin action
- "monitoring" - checking platform health

For COUNSELORS, intents:
- "case_management" - reviewing assigned cases
- "patient_insights" - asking about specific patient
- "clinical_recommendation" - seeking treatment suggestions

Return JSON: {{"intent": "...", "confidence": 0.0-1.0, "reasoning": "..."}}
"""
            
            intent_response = await generate_response(
                history=[{"role": "user", "content": intent_prompt}],
                model="gemini_google",
                temperature=0.3,
            )
            
            # Parse intent (simplified - in production, use JSON parsing)
            import json
            try:
                intent_data = json.loads(intent_response)
                state.intent = intent_data.get("intent", "unknown")
                state.intent_confidence = intent_data.get("confidence", 0.5)
            except:
                # Fallback
                state.intent = "emotional_support" if state.user_role == "user" else "analytics_query"
                state.intent_confidence = 0.5
            
            # Update conversation state with new intent
            if state.intent:
                conv_state.last_intent = state.intent
                conv_state.intent_changes.append(state.intent)
            
            state.agents_invoked.append("intent_classifier")
            
            elapsed = time.time() - start_time
            self.activity_logger.log_info(
                agent="Aika",
                message=f"Intent classified: {state.intent}",
                details={
                    "intent": state.intent,
                    "confidence": state.intent_confidence,
                    "cached": False,
                    "gemini_calls_total": conv_state.gemini_calls_made,
                    "efficiency_score": conv_state.efficiency_score,
                    "duration_ms": int(elapsed * 1000),
                }
            )
            
            logger.info(
                f"ðŸ“‹ Intent classified: {state.intent} (confidence: {state.intent_confidence:.2f}) "
                f"[Gemini calls: {conv_state.gemini_calls_made}, efficiency: {conv_state.efficiency_score:.1%}]"
            )
            
        except Exception as e:
            logger.error(f"âŒ Intent classification error: {e}")
            state.errors.append(f"Intent classification failed: {str(e)}")
            state.intent = "fallback"
        
        state.processing_time_ms = (time.time() - start_time) * 1000
        return state
    
    def _route_by_role(self, state: AikaState) -> str:
        """
        Route to appropriate agent based on user role and intent.
        
        Returns next node name.
        """
        role = state.user_role
        intent = state.intent
        
        # Student routing
        if role == "user":
            return "student_triage"
        
        # Admin routing
        elif role == "admin":
            if intent and ("analytics" in intent or "query" in intent):
                return "admin_analytics"
            else:
                return "admin_actions"
        
        # Counselor routing
        elif role == "counselor":
            return "counselor_cases"
        
        # Fallback
        return "student_triage"
    
    async def _student_triage(self, state: AikaState) -> AikaState:
        """
        Safety Triage Agent - assess risk level.
        
        REFACTORED: Now uses real STA graph with full workflow:
        - PII redaction (apply_redaction node)
        - Risk assessment (assess_risk node) - OPTIMIZED with Gemini + caching
        - Intent classification (decide_routing node)
        
        OPTIMIZED: Passes conversation context for smart caching.
        """
        try:
            self.activity_logger.log_agent_start("STA", "Analyzing message for safety concerns and risk factors")
            
            # Get conversation state for context
            # Ensure conversation_id exists
            if not state.conversation_id:
                state.conversation_id = f"conv_{state.user_id}_{int(time.time())}"
            
            conv_state = self._get_conversation_state(state.conversation_id, state.user_id)
            
            # Call STA service with full graph execution
            sta_state = await self.sta_service.execute(
                user_id=state.user_id,
                session_id=state.session_id or f"session_{state.user_id}_{int(time.time())}",
                user_hash=str(state.user_id),  # Use user_id as hash for now
                message=state.message,
                conversation_id=None,  # STA expects int | None, we have str, so pass None
            )
            
            # Extract results from STA state
            risk_level = sta_state.get("risk_level", "low")
            risk_score = sta_state.get("risk_score", 0.0)
            intent = sta_state.get("intent", "general")
            
            # Update conversation state with assessment results
            conv_state.update_after_assessment(
                risk_level=risk_level,
                risk_score=risk_score,
                intent=intent,
                skipped=False,
                gemini_called=sta_state.get("gemini_used", True),
            )
            
            state.triage_result = {
                "risk_level": risk_level,
                "risk_score": risk_score,
                "risk_factors": sta_state.get("risk_factors", []),
                "intent": intent,
                "confidence": sta_state.get("confidence", 0.0),
                "redacted_message": sta_state.get("redacted_message", state.message),
                "execution_path": sta_state.get("execution_path", []),
            }
            
            state.risk_level = sta_state.get("risk_level", "low")
            state.risk_factors = sta_state.get("risk_factors", [])
            state.intent = sta_state.get("intent", "general")
            state.agents_invoked.append("STA")
            
            # Log risk assessment
            self.activity_logger.log_risk_assessment(
                risk_level=state.risk_level or "low",
                risk_score=sta_state.get("risk_score", 0.0),
                risk_factors=state.risk_factors
            )
            
            self.activity_logger.log_agent_complete(
                "STA", 
                f"Risk assessment complete: {state.risk_level}",
                {"intent": state.intent, "execution_path": sta_state.get("execution_path", [])}
            )
            
        except Exception as e:
            self.activity_logger.log_agent_error("STA", "Safety triage failed", e)
            logger.error(f"STA error: {e}", exc_info=True)
            state.errors.append(f"Safety triage failed: {str(e)}")
            state.risk_level = "low"  # Default to "low" instead of "unknown" (must be valid Literal)
        
        return state
    
    def _check_crisis(self, state: AikaState) -> str:
        """Check if escalation is needed based on risk level"""
        if state.risk_level in ["high", "critical"]:
            return "crisis"
        return "no_crisis"
    
    async def _student_coaching(self, state: AikaState) -> AikaState:
        """
        Support Coach Agent - provide therapeutic support.
        
        REFACTORED: Now uses real SCA graph with full workflow:
        - Intervention type determination (determine_intervention_type node)
        - Plan generation (generate_plan node)
        - Safety review (safety_review node)
        - Plan persistence (persist_plan node)
        """
        try:
            self.activity_logger.log_agent_start("SCA", "Generating personalized support and intervention plan")
            
            # Extract triage data
            triage_result = state.triage_result or {}
            triage_assessment_id = triage_result.get("assessment_id")
            
            # Ensure valid risk_level (default to "low" if None)
            risk_level = state.risk_level or "low"
            intent = state.intent or "general"
            
            # Call SCA service with full graph execution
            sca_state = await self.sca_service.execute(
                user_id=state.user_id,
                session_id=state.session_id or f"session_{state.user_id}_{int(time.time())}",
                user_hash=str(state.user_id),
                message=state.message,
                conversation_id=None,  # SCA expects int | None, we have str, so pass None
                severity=risk_level,
                intent=intent,
                triage_assessment_id=triage_assessment_id,
            )
            
            # Extract results from SCA state
            state.coaching_result = {
                "intervention_type": sca_state.get("intervention_type"),
                "intervention_plan_id": sca_state.get("intervention_plan_id"),
                "should_intervene": sca_state.get("should_intervene", False),
                "safety_approved": sca_state.get("safety_approved", True),
                "execution_path": sca_state.get("execution_path", []),
            }
            
            # Build response with intervention plan if created
            if sca_state.get("intervention_plan_id"):
                state.response = self._build_coaching_response_with_plan(
                    sca_state.get("intervention_type") or "general_coping",
                    sca_state.get("intervention_plan_id") or 0
                )
                state.intervention_plan = {"id": sca_state.get("intervention_plan_id")}
                
                # Log intervention creation
                if sca_state.get("intervention_plan_id") and sca_state.get("intervention_type"):
                    self.activity_logger.log_intervention_created(
                        plan_id=sca_state.get("intervention_plan_id"),
                        intervention_type=sca_state.get("intervention_type")
                    )
            else:
                state.response = self._build_generic_coaching_response(state.risk_level or "low")
            
            state.agents_invoked.append("SCA")
            
            self.activity_logger.log_agent_complete(
                "SCA",
                "Coaching response generated",
                {
                    "has_plan": sca_state.get("intervention_plan_id") is not None,
                    "execution_path": sca_state.get("execution_path", [])
                }
            )
            
        except Exception as e:
            self.activity_logger.log_agent_error("SCA", "Coaching generation failed", e)
            logger.error(f"SCA error: {e}", exc_info=True)
            state.errors.append(f"Coaching failed: {str(e)}")
            state.response = AIKA_GREETINGS["student"]
        
        return state
    
    async def _student_escalation(self, state: AikaState) -> AikaState:
        """
        Service Desk Agent - escalate crisis to human counselor.
        
        REFACTORED: Now uses real SDA graph with full workflow:
        - Case creation (create_case node)
        - SLA calculation (calculate_sla node)
        - Auto-assignment (auto_assign node)
        - Notification (notify_counsellor node)
        """
        try:
            self.activity_logger.log_agent_start("SDA", "Creating crisis case and assigning counselor")
            
            # Extract triage data
            triage_result = state.triage_result or {}
            triage_assessment_id = triage_result.get("assessment_id")
            risk_score = triage_result.get("risk_score", 0.0)
            
            # Call SDA service with full graph execution
            sda_state = await self.sda_service.execute(
                user_id=state.user_id,
                session_id=state.session_id or f"session_{state.user_id}_{int(time.time())}",
                user_hash=str(state.user_id),
                message=state.message,
                conversation_id=None,  # SDA expects int | None
                severity=state.risk_level or "high",
                intent=state.intent or "crisis",
                risk_score=risk_score,
                triage_assessment_id=triage_assessment_id,
            )
            
            # Extract results from SDA state
            state.service_result = {
                "case_id": sda_state.get("case_id"),
                "case_created": sda_state.get("case_created", False),
                "sla_hours": sda_state.get("sla_hours"),
                "sla_breach_at": sda_state.get("sla_breach_at"),
                "assigned_counsellor_id": sda_state.get("assigned_counsellor_id"),
                "notification_sent": sda_state.get("notification_sent", False),
                "execution_path": sda_state.get("execution_path", []),
            }
            
            state.escalation_needed = True
            state.escalation_reason = "High-risk crisis detected"
            state.actions_taken.append("Created urgent case for counselor")
            state.agents_invoked.append("SDA")
            
            # Build crisis response with case ID
            if sda_state.get("case_created"):
                state.response = self._build_crisis_response(sda_state.get("case_id"))
                
                # Log case creation
                if sda_state.get("case_id"):
                    self.activity_logger.log_case_created(
                        case_id=sda_state.get("case_id"),
                        severity=state.risk_level or "critical",
                        sla_hours=sda_state.get("sla_hours", 0)
                    )
            else:
                state.response = self._build_crisis_fallback_response()
                self.activity_logger.log_warning("SDA", "Case creation failed, using fallback response")
            
            self.activity_logger.log_agent_complete(
                "SDA",
                "Crisis escalation complete",
                {
                    "case_id": sda_state.get("case_id"),
                    "assigned_counsellor_id": sda_state.get("assigned_counsellor_id"),
                    "execution_path": sda_state.get("execution_path", [])
                }
            )
            
        except Exception as e:
            self.activity_logger.log_agent_error("SDA", "Crisis escalation failed", e)
            logger.error(f"SDA error: {e}", exc_info=True)
            state.errors.append(f"Escalation failed: {str(e)}")
            state.response = self._build_crisis_fallback_response()
        
        return state
    
    async def _admin_analytics(self, state: AikaState) -> AikaState:
        """
        Insights Agent - provide analytics for admin.
        
        REFACTORED: Uses simplified LLM for now.
        Note: Full IA integration requires question_id mapping from admin query to predefined analytics questions.
        """
        try:
            logger.info("Providing admin analytics...")
            
            # Use LLM to handle admin query
            system_prompt = AIKA_SYSTEM_PROMPTS["admin"]
            
            analytics_response = await generate_response(
                history=[{
                    "role": "system",
                    "content": system_prompt
                }, {
                    "role": "user",
                    "content": state.message
                }],
                model="gemini_google",
                temperature=0.3,
            )
            
            state.insights_result = {
                "response": analytics_response,
                "note": "Analytics via LLM (full IA integration pending)"
            }
            state.admin_query_result = state.insights_result
            state.response = analytics_response
            state.agents_invoked.append("IA")
            
            logger.info("Admin analytics provided")
            
        except Exception as e:
            logger.error(f"IA error: {e}", exc_info=True)
            state.errors.append(f"Analytics failed: {str(e)}")
            state.response = "Maaf, terjadi error saat mengambil analytics."
        
        return state
    
    async def _admin_actions(self, state: AikaState) -> AikaState:
        """
        Service Desk Agent - execute admin actions.
        
        REFACTORED: Uses simplified LLM for admin commands.
        Note: Dedicated admin action handler should be implemented for production.
        """
        try:
            logger.info("Processing admin action...")
            
            # Use LLM to handle admin command
            system_prompt = AIKA_SYSTEM_PROMPTS["admin"]
            
            action_response = await generate_response(
                history=[{
                    "role": "system",
                    "content": system_prompt
                }, {
                    "role": "user",
                    "content": state.message
                }],
                model="gemini_google",
                temperature=0.3,
            )
            
            state.service_result = {
                "response": action_response,
                "note": "Admin action via LLM (dedicated handler pending)"
            }
            state.admin_action_result = state.service_result
            state.response = action_response
            state.agents_invoked.append("SDA")
            
            logger.info("Admin action processed")
            
        except Exception as e:
            logger.error(f"SDA error: {e}", exc_info=True)
            state.errors.append(f"Admin action failed: {str(e)}")
            state.response = "Maaf, terjadi error saat menjalankan perintah."
        
        return state
    
    async def _counselor_cases(self, state: AikaState) -> AikaState:
        """
        Service Desk Agent - provide case management for counselors.
        
        REFACTORED: Uses SDA service methods for case listing.
        Note: Full integration requires proper case query API.
        """
        try:
            logger.info("Providing counselor case management...")
            
            # Use LLM to handle counselor query
            system_prompt = AIKA_SYSTEM_PROMPTS["counselor"]
            
            cases_response = await generate_response(
                history=[{
                    "role": "system",
                    "content": system_prompt
                }, {
                    "role": "user",
                    "content": state.message
                }],
                model="gemini_google",
                temperature=0.3,
            )
            
            state.service_result = {
                "response": cases_response,
                "note": "Case management via LLM"
            }
            state.counselor_cases_result = state.service_result
            state.response = cases_response
            state.agents_invoked.append("SDA")
            
            logger.info("Case management info provided")
            
        except Exception as e:
            logger.error(f"SDA error: {e}", exc_info=True)
            state.errors.append(f"Case management failed: {str(e)}")
            state.response = self._build_fallback_response("counselor")
        
        return state
    
    async def _counselor_insights(self, state: AikaState) -> AikaState:
        """
        Insights Agent - provide clinical insights for counselors.
        
        REFACTORED: Uses simplified LLM for clinical insights.
        Note: Full IA integration requires question_id mapping.
        """
        try:
            logger.info("Providing counselor insights...")
            
            # Use LLM to provide clinical insights
            system_prompt = AIKA_SYSTEM_PROMPTS["counselor"]
            
            insights_response = await generate_response(
                history=[{
                    "role": "system",
                    "content": system_prompt
                }, {
                    "role": "user",
                    "content": state.message
                }],
                model="gemini_google",
                temperature=0.3,
            )
            
            state.insights_result = {
                "response": insights_response,
                "note": "Clinical insights via LLM"
            }
            state.counselor_insights_result = state.insights_result
            state.response = insights_response
            state.agents_invoked.append("IA")
            
            logger.info("Clinical insights provided")
            
        except Exception as e:
            logger.error(f"Insights error: {e}", exc_info=True)
            state.errors.append(f"Insights failed: {str(e)}")
            state.response = self._build_fallback_response("counselor")
        
        return state
    
    async def _synthesize_response(self, state: AikaState) -> AikaState:
        """
        Final synthesis - ensure response maintains Aika's personality.
        
        This node ensures all responses sound like Aika, regardless of
        which specialized agents were invoked.
        """
        try:
            logger.info("âœ¨ Synthesizing final Aika response...")
            
            # If response already generated by specialized agent, just validate tone
            if state.response:
                # Response already exists, just log
                logger.info("âœ¨ Using response from specialized agent")
            else:
                # Generate fallback response
                state.response = self._build_fallback_response(state.user_role)
            
            # Add actions summary if any actions were taken
            if state.actions_taken and state.response:
                actions_summary = "\n\nðŸ”§ **Actions taken:**\n" + "\n".join(
                    f"- {action}" for action in state.actions_taken
                )
                state.response += actions_summary
            
            # Add escalation notice if needed
            if state.escalation_needed and state.response:
                state.response += "\n\nâš ï¸ **Case escalated to human counselor for immediate attention.**"
            
        except Exception as e:
            logger.error(f"âŒ Response synthesis error: {e}")
            state.response = self._generate_error_response(state)
        
        return state
    
    async def _background_analytics(self, state: AikaState) -> AikaState:
        """
        Background analytics - run after response (non-blocking).
        
        IA collects anonymized metrics for research and platform improvement.
        """
        try:
            logger.info("ðŸ“ˆ Running background analytics...")
            
            # Call IA service to log anonymized metrics (fire-and-forget)
            await self.ia_service.log_interaction_metrics(
                user_role=state.user_role,
                intent=state.intent,
                risk_level=state.risk_level,
                agents_invoked=state.agents_invoked,
                processing_time_ms=state.processing_time_ms,
            )
            
            logger.info("ðŸ“ˆ Background analytics completed")
            
        except Exception as e:
            logger.error(f"âš ï¸ Background analytics error (non-critical): {e}")
        
        return state
    
    # ===================== HELPER METHODS =====================
    
    async def _get_user_context(self, user_id: int) -> Dict:
        """Fetch user context from database"""
        try:
            from app.models import User
            from app.domains.mental_health.models import PlayerWellnessState, Conversation
            from sqlalchemy import select, func
            
            # Get user basic info
            result = await self.db.execute(select(User).where(User.id == user_id))
            user = result.scalar_one_or_none()
            
            if not user:
                return {"user_id": user_id, "has_history": False}
            
            # Get wellness state
            result = await self.db.execute(
                select(PlayerWellnessState).where(PlayerWellnessState.user_id == user_id)
            )
            wellness = result.scalar_one_or_none()
            
            # Count conversations
            result = await self.db.execute(
                select(func.count(Conversation.id))
                .where(Conversation.user_id == user_id)
            )
            conversation_count = result.scalar() or 0
            
            context = {
                "user_id": user_id,
                "preferred_name": user.preferred_name or user.first_name or user.name,
                "has_history": conversation_count > 0,
                "conversation_count": conversation_count,
            }
            
            if wellness:
                context.update({
                    "harmony_score": wellness.harmony_score,
                    "current_streak": wellness.current_streak,
                    "wellness_level": "good" if wellness.harmony_score >= 70 else "needs_attention",
                })
            
            return context
            
        except Exception as e:
            logger.warning(f"âš ï¸ Failed to fetch user context: {e}")
            return {
                "user_id": user_id,
                "has_history": False,
            }
    
    # ===================== HELPER METHODS =====================
    
    def _build_coaching_response_with_plan(self, intervention_type: str, plan_id: int) -> str:
        """Build response text when intervention plan is created."""
        return (
            f"Aku sudah menyiapkan rencana dukungan untuk kamu. "
            f"Rencana ini akan membantu kamu mengatasi situasi ini dengan lebih baik. "
            f"Kamu bisa melihat detailnya di dashboard intervention plans."
        )
    
    def _build_generic_coaching_response(self, risk_level: str) -> str:
        """Build generic coaching response without plan."""
        if risk_level in ["high", "critical"]:
            return (
                "Aku mendengarkan kamu dan peduli dengan apa yang kamu rasakan. "
                "Situasimu terdengar berat. Aku sarankan untuk berbicara dengan konselor profesional. "
                "Apakah kamu mau aku hubungkan dengan konselor?"
            )
        else:
            return (
                "Terima kasih sudah berbagi dengan aku. "
                "Aku di sini untuk mendengarkan dan mendukungmu. "
                "Bagaimana perasaanmu sekarang?"
            )
    
    def _build_crisis_response(self, case_id: Optional[int] = None) -> str:
        """Build response for crisis escalation."""
        if case_id:
            return (
                "Aku sangat peduli dengan keselamatanmu. "
                "Aku sudah menghubungkan kamu dengan tim konselor kami (Case #{case_id}). "
                "Mereka akan segera menghubungimu. Kamu tidak sendirian. "
                "Jika ini darurat, segera hubungi 119 atau pergi ke IGD terdekat."
            )
        else:
            return self._build_crisis_fallback_response()
    
    def _build_crisis_fallback_response(self) -> str:
        """Fallback crisis response when case creation fails."""
        return (
            "Aku sangat peduli dengan keselamatanmu. "
            "Mohon segera hubungi konselor atau layanan krisis: "
            "ðŸ†˜ Hotline: 119 (24/7) "
            "ðŸ¥ Atau kunjungi IGD terdekat "
            "Kamu tidak sendirian, dan bantuannya tersedia."
        )
    
    def _build_fallback_response(self, role: str) -> str:
        """Build fallback response when agents fail."""
        if role == "user":
            return (
                "Maaf, aku sedang mengalami kendala teknis. "
                "Tapi aku tetap di sini untukmu. Coba ulangi pesanmu?"
            )
        elif role == "admin":
            return "Sistem sedang mengalami kendala. Silakan coba lagi."
        else:
            return "I'm experiencing technical difficulties. Please try again."
    
    def _generate_error_response(self, state: AikaState) -> str:
        """Generate error response"""
        role = state.user_role
        
        if role == "user":
            return "Maaf, aku sedang mengalami kendala teknis. Tapi aku tetap di sini untukmu. Coba ulangi pesanmu?"
        elif role == "admin":
            return f"Error occurred: {', '.join(state.errors)}"
        else:
            return "I'm experiencing technical difficulties. Please try again."
    
    # ===================== PUBLIC API =====================
    
    def set_activity_callback(self, callback: Callable):
        """Set callback for real-time activity broadcasting (e.g., WebSocket)"""
        self.activity_logger.set_callback(callback)
    
    def get_activity_logs(self) -> List[Dict]:
        """Get all activity logs from current session"""
        return self.activity_logger.get_activities()
    
    def clear_activity_logs(self):
        """Clear activity log history"""
        self.activity_logger.clear()
    
    async def process_message(
        self,
        user_id: int,
        user_role: Literal["user", "counselor", "admin"],
        message: str,
        session_id: Optional[str] = None,
        conversation_history: Optional[List[Dict[str, str]]] = None,
    ) -> Dict:
        """
        Main entry point - process user message through Aika.
        
        Args:
            user_id: User ID
            user_role: User's role (user=student, counselor, admin)
            message: User's message
            session_id: Optional session ID for continuity
            conversation_history: Optional conversation history
        
        Returns:
            Dict with response and metadata
        """
        start_time = time.time()
        
        # Clear previous activity logs
        self.activity_logger.clear()
        
        # Generate conversation ID if not provided (use session_id as conversation_id for now)
        conversation_id = session_id or f"conv_{user_id}_{int(time.time())}"
        
        # Initialize state
        initial_state = AikaState(
            user_id=user_id,
            user_role=user_role,
            message=message,
            session_id=session_id or f"session_{user_id}_{int(time.time())}",
            conversation_id=conversation_id,
            conversation_history=conversation_history or [],
        )
        
        try:
            self.activity_logger.log_info("Aika", f"Processing message from {user_role}", {"user_id": user_id})
            
            # Run through orchestration graph
            config = {"configurable": {"thread_id": initial_state.session_id}}
            final_state = await self.graph.ainvoke(initial_state, config)
            
            # Calculate total processing time
            total_time_ms = (time.time() - start_time) * 1000
            
            # âœ… FIX: final_state is a dict, not an object - access with dict keys
            agents_invoked = final_state.get("agents_invoked", [])
            
            logger.info(
                f"âœ… Aika processed message in {total_time_ms:.2f}ms "
                f"(agents: {', '.join(agents_invoked)})"
            )
            
            # Return response
            result = {
                "success": True,
                "response": final_state.get("response", ""),
                "metadata": {
                    "session_id": final_state.get("session_id", ""),
                    "user_role": final_state.get("user_role", ""),
                    "intent": final_state.get("intent", ""),
                    "agents_invoked": agents_invoked,
                    "processing_time_ms": total_time_ms,
                    "risk_level": final_state.get("risk_level", ""),
                    "escalation_needed": final_state.get("escalation_needed", False),
                    "actions_taken": final_state.get("actions_taken", []),
                },
                "errors": final_state.get("errors") if final_state.get("errors") else None,
                "activity_logs": self.get_activity_logs(),  # Include activity logs
            }
            
            # Include intervention plan if created
            if final_state.get("intervention_plan"):
                result["intervention_plan"] = final_state["intervention_plan"]
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ Aika orchestration error: {e}", exc_info=True)
            
            # Calculate processing time even for errors
            total_time_ms = (time.time() - start_time) * 1000
            
            return {
                "success": False,
                "error": str(e),
                "response": self._generate_error_response(initial_state),
                "metadata": {
                    "session_id": initial_state.session_id,
                    "user_role": initial_state.user_role,
                    "intent": initial_state.intent or "unknown",
                    "agents_invoked": initial_state.agents_invoked,
                    "processing_time_ms": total_time_ms,
                    "risk_level": initial_state.risk_level or "low",  # Use "low" instead of "unknown"
                    "escalation_needed": False,
                    "actions_taken": [],
                },
            }

    async def process_message_with_tools(
        self,
        user_id: int,
        user_role: Literal["user", "counselor", "admin"],
        message: str,
        session_id: Optional[str] = None,
        conversation_history: Optional[List[Dict[str, str]]] = None,
    ) -> Dict:
        """
        NEW TOOL-CALLING ENTRY POINT (Oct 2025 Optimization).
        
        Uses Gemini function calling to intelligently decide when to activate agents.
        Replaces always-run-agents pattern with conditional invocation.
        
        Architecture:
        1. Call Gemini with tools (1-2s)
        2. If Gemini requests tools â†’ Execute LangGraph agents
        3. If no tools â†’ Direct conversational response
        
        Expected Performance:
        - Casual chat: 1.2s (no agents)
        - Plan request: 6.5s (SCA only)
        - Crisis: 5.5s (STA + SDA)
        - Average: 1.8s (83% improvement from 10.7s)
        
        Args:
            user_id: User ID
            user_role: User's role (user=student, counselor, admin)
            message: User's message
            session_id: Optional session ID for continuity
            conversation_history: Optional conversation history
        
        Returns:
            Dict with response and metadata (same format as process_message)
        """
        from google import genai
        from google.genai import types
        from .tool_definitions import get_gemini_tools
        from .identity import AIKA_SYSTEM_PROMPTS
        import uuid
        
        start_time = time.time()
        
        # Clear previous activity logs
        self.activity_logger.clear()
        
        # Generate IDs
        conversation_id = session_id or f"conv_{user_id}_{int(time.time())}"
        session_id = session_id or f"session_{user_id}_{int(time.time())}"
        
        # Track which agents were invoked
        agents_invoked = []
        risk_level = "low"
        escalation_needed = False
        actions_taken = []
        intervention_plan = None
        errors = []
        
        try:
            # Get Aika's personality based on role
            system_instruction = AIKA_SYSTEM_PROMPTS.get(user_role, AIKA_SYSTEM_PROMPTS["student"])
            
            self.activity_logger.log_info(
                "Aika", 
                f"ðŸ§  Tool-calling mode: Processing message from {user_role}",
                {"user_id": user_id, "session_id": session_id}
            )
            
            # Get conversation history from Redis if not provided
            if not conversation_history:
                conversation_history = await self._get_conversation_history(session_id)
                if conversation_history:
                    self.activity_logger.log_info(
                        "Aika",
                        f"ðŸ“œ Loaded {len(conversation_history)} messages from Redis",
                        {"session_id": session_id}
                    )
            
            # Prepare conversation history for Gemini
            history_contents = []
            if conversation_history:
                for msg in conversation_history:
                    role = "user" if msg.get("role") == "user" else "model"
                    history_contents.append(
                        types.Content(
                            role=role,
                            parts=[types.Part(text=msg.get("content", ""))]
                        )
                    )
            
            # Add current message
            history_contents.append(
                types.Content(
                    role="user",
                    parts=[types.Part(text=message)]
                )
            )
            
            # Call Gemini with tools
            client = genai.Client(api_key=os.getenv("GOOGLE_GENAI_API_KEY"))
            
            self.activity_logger.log_info(
                "Aika",
                "ðŸ“¡ Calling Gemini with 5 agent tools",
                {"message_length": len(message)}
            )
            
            response = client.models.generate_content(
                model="gemini-2.0-flash-exp",
                contents=history_contents,
                config=types.GenerateContentConfig(
                    system_instruction=system_instruction,
                    temperature=0.7,
                    tools=get_gemini_tools()
                )
            )
            
            # Check if Gemini requested tool calls
            if response.candidates and response.candidates[0].content.parts:
                first_part = response.candidates[0].content.parts[0]
                
                if hasattr(first_part, 'function_call') and first_part.function_call:
                    # Tool execution path
                    self.activity_logger.log_info(
                        "Aika",
                        f"ðŸ”§ Gemini requested tool: {first_part.function_call.name}",
                        {"args": dict(first_part.function_call.args)}
                    )
                    
                    # Handle tool calls (can be multiple in sequence)
                    final_response, tool_metadata = await self._handle_tool_calls(
                        response=response,
                        user_id=user_id,
                        session_id=session_id,
                        conversation_id=conversation_id,
                        client=client,
                        history_contents=history_contents,
                        system_instruction=system_instruction
                    )
                    
                    # Update metadata from tool execution
                    agents_invoked = tool_metadata.get("agents_invoked", [])
                    risk_level = tool_metadata.get("risk_level", "low")
                    escalation_needed = tool_metadata.get("escalation_needed", False)
                    actions_taken = tool_metadata.get("actions_taken", [])
                    intervention_plan = tool_metadata.get("intervention_plan")
                    errors = tool_metadata.get("errors", [])
                    
                else:
                    # Direct conversational response (no tools needed)
                    final_response = response.text
                    self.activity_logger.log_info(
                        "Aika",
                        "ðŸ’¬ Direct response (no agents needed)",
                        {"response_length": len(final_response)}
                    )
            else:
                # Fallback: empty response
                final_response = "Maaf, aku mengalami kesulitan memproses pesan kamu. Bisa coba lagi?"
                errors.append("Empty response from Gemini")
            
            # Calculate total processing time
            total_time_ms = (time.time() - start_time) * 1000
            
            logger.info(
                f"âœ… Aika (tool-calling) processed message in {total_time_ms:.2f}ms "
                f"(agents: {', '.join(agents_invoked) if agents_invoked else 'none'})"
            )
            
            # Save conversation to Redis for future context
            await self._save_conversation_history(session_id, message, final_response)
            
            # Return response in same format as process_message
            result = {
                "success": True,
                "response": final_response,
                "metadata": {
                    "session_id": session_id,
                    "user_role": user_role,
                    "intent": "tool_calling_mode",  # New architecture marker
                    "agents_invoked": agents_invoked,
                    "processing_time_ms": total_time_ms,
                    "risk_level": risk_level,
                    "escalation_needed": escalation_needed,
                    "actions_taken": actions_taken,
                },
                "errors": errors if errors else None,
                "activity_logs": self.get_activity_logs(),
            }
            
            # Include intervention plan if created
            if intervention_plan:
                result["intervention_plan"] = intervention_plan
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ Aika tool-calling error: {e}", exc_info=True)
            
            # Calculate processing time even for errors
            total_time_ms = (time.time() - start_time) * 1000
            
            # Generate empathetic error response
            error_response = (
                "Maaf ya, aku lagi mengalami sedikit kendala teknis. "
                "Kalau ini tentang krisis atau darurat, segera hubungi:\n\n"
                "ðŸš¨ **Hotline Crisis Centre UGM**: 0274-544571\n"
                "ðŸ“ž **Telepon/SMS**: 0851-0111-0800\n\n"
                "Aku akan siap membantu lagi sebentar lagi! ðŸ’™"
            )
            
            return {
                "success": False,
                "error": str(e),
                "response": error_response,
                "metadata": {
                    "session_id": session_id,
                    "user_role": user_role,
                    "intent": "tool_calling_mode",
                    "agents_invoked": agents_invoked,
                    "processing_time_ms": total_time_ms,
                    "risk_level": "unknown",
                    "escalation_needed": False,
                    "actions_taken": [],
                },
            }
    
    async def _handle_tool_calls(
        self,
        response,
        user_id: int,
        session_id: str,
        conversation_id: str,
        client,
        history_contents: List,
        system_instruction: str,
    ) -> tuple[str, Dict]:
        """
        Handle tool calls from Gemini (multi-turn execution).
        
        Process:
        1. Extract tool calls from response
        2. Execute each tool (run LangGraph agents)
        3. Send tool results back to Gemini
        4. Get final synthesized response
        
        Returns:
            Tuple of (final_response_text, metadata_dict)
        """
        from google.genai import types
        from .tool_definitions import get_gemini_tools
        import uuid
        
        # Track metadata across tool executions
        agents_invoked = []
        risk_level = "low"
        escalation_needed = False
        actions_taken = []
        intervention_plan = None
        errors = []
        
        # Initialize final_text with fallback
        final_text = "Maaf, aku kesulitan memberikan respons yang tepat."
        
        # Multi-turn tool execution loop
        max_turns = 5  # Prevent infinite loops
        current_response = response
        
        for turn in range(max_turns):
            # Check if current response has tool calls
            if not current_response.candidates or not current_response.candidates[0].content.parts:
                break
            
            first_part = current_response.candidates[0].content.parts[0]
            
            if not hasattr(first_part, 'function_call') or not first_part.function_call:
                # No more tool calls, get final text response
                if hasattr(first_part, 'text'):
                    final_text = first_part.text
                else:
                    final_text = "Maaf, aku kesulitan memberikan respons yang tepat."
                break
            
            # Execute tool
            function_call = first_part.function_call
            tool_name = function_call.name
            tool_args = dict(function_call.args)
            
            self.activity_logger.log_info(
                "Aika",
                f"ðŸ”§ Executing tool: {tool_name}",
                {"args": tool_args, "turn": turn + 1}
            )
            
            try:
                # Execute tool and get result
                tool_result = await self._execute_tool(
                    tool_name=tool_name,
                    args=tool_args,
                    user_id=user_id,
                    session_id=session_id,
                    conversation_id=conversation_id,
                    message=history_contents[-1].parts[0].text  # Original user message
                )
                
                # Update metadata
                if "agent" in tool_result:
                    agents_invoked.append(tool_result["agent"])
                if "risk_level" in tool_result:
                    risk_level = tool_result["risk_level"]
                if "escalation_needed" in tool_result:
                    escalation_needed = tool_result["escalation_needed"]
                if "actions_taken" in tool_result:
                    actions_taken.extend(tool_result["actions_taken"])
                if "intervention_plan" in tool_result:
                    intervention_plan = tool_result["intervention_plan"]
                if "error" in tool_result:
                    errors.append(tool_result["error"])
                
                # Send tool result back to Gemini
                history_contents.append(current_response.candidates[0].content)
                history_contents.append(
                    types.Content(
                        role="user",
                        parts=[
                            types.Part(
                                function_response=types.FunctionResponse(
                                    name=tool_name,
                                    response={"result": tool_result}
                                )
                            )
                        ]
                    )
                )
                
                # Get next response from Gemini (may contain more tool calls or final answer)
                current_response = client.models.generate_content(
                    model="gemini-2.0-flash-exp",
                    contents=history_contents,
                    config=types.GenerateContentConfig(
                        system_instruction=system_instruction,
                        temperature=0.7,
                        tools=get_gemini_tools()
                    )
                )
                
            except Exception as e:
                logger.error(f"âŒ Tool execution error ({tool_name}): {e}", exc_info=True)
                errors.append(f"Tool {tool_name} failed: {str(e)}")
                
                # Send error back to Gemini
                history_contents.append(current_response.candidates[0].content)
                history_contents.append(
                    types.Content(
                        role="user",
                        parts=[
                            types.Part(
                                function_response=types.FunctionResponse(
                                    name=tool_name,
                                    response={"error": str(e)}
                                )
                            )
                        ]
                    )
                )
                
                # Try to get response from Gemini despite tool failure
                try:
                    current_response = client.models.generate_content(
                        model="gemini-2.0-flash-exp",
                        contents=history_contents,
                        config=types.GenerateContentConfig(
                            system_instruction=system_instruction,
                            temperature=0.7,
                            tools=get_gemini_tools()
                        )
                    )
                except:
                    # Complete failure, return error message
                    final_text = (
                        "Maaf ya, aku mengalami kesulitan teknis. "
                        "Kalau ini urgent, hubungi Crisis Centre UGM: 0274-544571 ðŸ’™"
                    )
                    break
        else:
            # Max turns reached
            final_text = "Maaf, aku membutuhkan terlalu banyak langkah untuk menjawab. Bisa coba tanya dengan cara lain?"
            errors.append("Max tool execution turns reached")
        
        # Compile metadata
        metadata = {
            "agents_invoked": agents_invoked,
            "risk_level": risk_level,
            "escalation_needed": escalation_needed,
            "actions_taken": actions_taken,
            "intervention_plan": intervention_plan,
            "errors": errors,
        }
        
        return final_text, metadata
    
    async def _execute_tool(
        self,
        tool_name: str,
        args: Dict,
        user_id: int,
        session_id: str,
        conversation_id: str,
        message: str,
    ) -> Dict:
        """
        Execute a specific tool by running corresponding LangGraph agent or DB query.
        
        Args:
            tool_name: Name of tool to execute (from tool_definitions.py)
            args: Tool arguments from Gemini
            user_id: User ID
            session_id: Session ID for continuity
            conversation_id: Conversation ID for DB relations
            message: Original user message
        
        Returns:
            Dict with tool execution results and metadata
        """
        import uuid
        
        if tool_name == "run_safety_triage_agent":
            # Execute STA LangGraph pipeline
            reason = args.get("reason", "User message indicates potential risk")
            urgency = args.get("urgency_override", "normal")
            
            self.activity_logger.log_info(
                "STA",
                f"ðŸš¨ Running safety triage: {reason}",
                {"urgency": urgency}
            )
            
            try:
                # Run STA graph
                sta_result = await self.sta_service.execute(
                    user_id=user_id,
                    session_id=session_id,
                    user_hash=str(user_id),
                    message=message,
                    conversation_id=None,  # Tool mode doesn't persist to conversation yet
                )
                
                severity = sta_result.get("severity", "unknown")
                recommended_action = sta_result.get("recommended_action", "")
                assessment_id = sta_result.get("assessment_id")
                
                self.activity_logger.log_info(
                    "STA",
                    f"âœ… Triage complete: {severity}",
                    {"assessment_id": assessment_id}
                )
                
                return {
                    "status": "completed",
                    "agent": "STA",
                    "severity": severity,
                    "risk_level": severity,
                    "recommended_action": recommended_action,
                    "assessment_id": assessment_id,
                    "escalation_needed": severity in ["high", "critical"],
                    "actions_taken": [f"Risk assessment completed (severity: {severity})"],
                }
                
            except Exception as e:
                logger.error(f"âŒ STA execution failed: {e}", exc_info=True)
                return {
                    "status": "failed",
                    "agent": "STA",
                    "error": str(e),
                    "risk_level": "unknown",
                }
        
        elif tool_name == "run_support_coach_agent":
            # Execute SCA LangGraph pipeline
            intervention_hint = args.get("intervention_hint", "general")
            severity_context = args.get("severity_context", "low")
            
            self.activity_logger.log_info(
                "SCA",
                f"ðŸ’™ Creating intervention plan: {intervention_hint}",
                {"severity": severity_context}
            )
            
            try:
                # Run SCA graph
                sca_result = await self.sca_service.execute(
                    user_id=user_id,
                    session_id=session_id,
                    user_hash=str(user_id),
                    message=message,
                    conversation_id=None,  # Tool mode doesn't persist to conversation yet
                    severity=severity_context,
                    intent=intervention_hint,
                )
                
                plan_id = sca_result.get("intervention_plan_id")
                plan_data = sca_result.get("plan_data", {})
                
                self.activity_logger.log_info(
                    "SCA",
                    f"âœ… Plan created: {plan_id}",
                    {"total_steps": plan_data.get("total_steps", 0)}
                )
                
                return {
                    "status": "completed",
                    "agent": "SCA",
                    "plan_id": plan_id,
                    "intervention_plan": plan_data,
                    "total_steps": plan_data.get("total_steps", 0),
                    "actions_taken": [f"Intervention plan created (ID: {plan_id})"],
                }
                
            except Exception as e:
                logger.error(f"âŒ SCA execution failed: {e}", exc_info=True)
                return {
                    "status": "failed",
                    "agent": "SCA",
                    "error": str(e),
                }
        
        elif tool_name == "run_service_desk_agent":
            # Execute SDA workflow (case creation)
            service_type = args.get("service_type", "counseling")
            priority = args.get("priority", "normal")
            
            self.activity_logger.log_info(
                "SDA",
                f"ðŸ“‹ Creating service desk case: {service_type}",
                {"priority": priority}
            )
            
            try:
                # Run SDA graph
                sda_result = await self.sda_service.execute(
                    user_id=user_id,
                    session_id=session_id,
                    user_hash=str(user_id),
                    message=message,
                    conversation_id=None,  # Tool mode doesn't persist to conversation yet
                    severity="high" if priority == "high" else "critical",
                    intent=service_type,
                )
                
                case_id = sda_result.get("case_id")
                status = sda_result.get("status", "open")
                
                self.activity_logger.log_info(
                    "SDA",
                    f"âœ… Case created: {case_id}",
                    {"status": status}
                )
                
                return {
                    "status": "completed",
                    "agent": "SDA",
                    "case_id": case_id,
                    "case_status": status,
                    "escalation_needed": True,
                    "actions_taken": [f"Service desk case created (ID: {case_id})"],
                }
                
            except Exception as e:
                logger.error(f"âŒ SDA execution failed: {e}", exc_info=True)
                return {
                    "status": "failed",
                    "agent": "SDA",
                    "error": str(e),
                }
        
        elif tool_name == "get_user_intervention_plans":
            # DB query for user's plans
            active_only = args.get("active_only", False)
            
            self.activity_logger.log_info(
                "Database",
                f"ðŸ“Š Fetching intervention plans (active_only={active_only})",
                {"user_id": user_id}
            )
            
            try:
                from sqlalchemy import select
                from app.domains.mental_health.models.interventions import InterventionPlanRecord
                
                query = select(InterventionPlanRecord).where(
                    InterventionPlanRecord.user_id == user_id
                )
                
                if active_only:
                    query = query.where(InterventionPlanRecord.status == "active")
                
                query = query.order_by(InterventionPlanRecord.created_at.desc())
                
                result = await self.db.execute(query)
                plans = result.scalars().all()
                
                plans_data = [
                    {
                        "id": plan.id,
                        "title": plan.title,
                        "status": plan.status,
                        "total_steps": plan.total_steps,
                        "completed_steps": plan.completed_steps,
                        "created_at": plan.created_at.isoformat(),
                    }
                    for plan in plans
                ]
                
                self.activity_logger.log_info(
                    "Database",
                    f"âœ… Found {len(plans_data)} plans",
                    {"active_only": active_only}
                )
                
                return {
                    "status": "completed",
                    "plans": plans_data,
                    "count": len(plans_data),
                }
                
            except Exception as e:
                logger.error(f"âŒ Failed to fetch plans: {e}", exc_info=True)
                return {
                    "status": "failed",
                    "error": str(e),
                    "plans": [],
                }
        
        elif tool_name == "get_mental_health_resources":
            # Fetch resources (from DB or hardcoded)
            category = args.get("category", "all")
            topic = args.get("topic")
            
            self.activity_logger.log_info(
                "Resources",
                f"ðŸ“š Fetching mental health resources: {category}",
                {"topic": topic}
            )
            
            try:
                # TODO: Replace with actual DB query when resources table exists
                # For now, return hardcoded resources
                resources = {
                    "crisis": [
                        {
                            "title": "Crisis Centre UGM",
                            "type": "hotline",
                            "contact": "0274-544571 atau 0851-0111-0800",
                            "available": "24/7",
                        },
                        {
                            "title": "Grhatama Pustaka (GMC)",
                            "type": "in_person",
                            "location": "Gedung Grhatama Pustaka UGM, Lantai 2",
                            "hours": "Senin-Jumat 08:00-15:00",
                        },
                    ],
                    "coping": [
                        {
                            "title": "Teknik Pernapasan 4-7-8",
                            "description": "Tarik napas 4 detik, tahan 7 detik, hembuskan 8 detik",
                            "type": "breathing",
                        },
                        {
                            "title": "Grounding 5-4-3-2-1",
                            "description": "Sebutkan 5 hal yang kamu lihat, 4 yang kamu dengar, 3 yang kamu sentuh, 2 yang kamu cium, 1 yang kamu rasakan",
                            "type": "grounding",
                        },
                    ],
                    "educational": [
                        {
                            "title": "Mengenal Kecemasan",
                            "url": "https://ugm.ac.id/kesehatan-mental",
                            "type": "article",
                        },
                    ],
                }
                
                if category == "all":
                    result_resources = []
                    for cat_resources in resources.values():
                        result_resources.extend(cat_resources)
                else:
                    result_resources = resources.get(category, [])
                
                # Filter by topic if provided
                if topic:
                    result_resources = [
                        r for r in result_resources
                        if topic.lower() in r.get("title", "").lower()
                        or topic.lower() in r.get("description", "").lower()
                    ]
                
                self.activity_logger.log_info(
                    "Resources",
                    f"âœ… Found {len(result_resources)} resources",
                    {"category": category, "topic": topic}
                )
                
                return {
                    "status": "completed",
                    "resources": result_resources,
                    "count": len(result_resources),
                }
                
            except Exception as e:
                logger.error(f"âŒ Failed to fetch resources: {e}", exc_info=True)
                return {
                    "status": "failed",
                    "error": str(e),
                }
        
        elif tool_name == "get_user_profile":
            # Fetch user information from database
            self.activity_logger.log_info(
                "Database",
                f"ðŸ“Š Fetching profile for user {user_id}",
                {"user_id": user_id}
            )
            
            try:
                from app.models import User
                from sqlalchemy import select
                
                query = select(User).where(User.id == user_id)
                result = await self.db.execute(query)
                user = result.scalar_one_or_none()
                
                if user:
                    self.activity_logger.log_info(
                        "Database",
                        f"âœ… Profile retrieved: {user.full_name}",
                        {"email": user.email}
                    )
                    
                    return {
                        "status": "completed",
                        "user_id": user.id,
                        "name": user.full_name or "Pengguna",
                        "email": user.email,
                        "role": user.role,
                        "registered_date": user.created_at.isoformat() if user.created_at else None,
                        "is_active": user.is_active,
                    }
                else:
                    return {
                        "status": "not_found",
                        "error": "User profile not found",
                    }
            
            except Exception as e:
                logger.error(f"âŒ Failed to fetch user profile: {e}", exc_info=True)
                return {
                    "status": "failed",
                    "error": str(e),
                }
        
        elif tool_name == "create_intervention_plan":
            # Create CBT intervention plan directly
            plan_title = args.get("plan_title", "Rencana Dukungan Mental")
            concern_type = args.get("concern_type", "general")
            severity = args.get("severity", "moderate")
            
            self.activity_logger.log_info(
                "SCA",
                f"ðŸ’™ Creating intervention plan: {plan_title}",
                {"concern": concern_type, "severity": severity}
            )
            
            try:
                # Call SCA service to generate structured plan
                sca_result = await self.sca_service.execute(
                    user_id=user_id,
                    session_id=session_id,
                    user_hash=str(user_id),
                    message=f"User needs help with {concern_type}: {plan_title}",
                    conversation_id=None,
                    severity=severity,
                    intent=concern_type,
                )
                
                plan_id = sca_result.get("intervention_plan_id")
                plan_data = sca_result.get("plan_data", {})
                
                self.activity_logger.log_info(
                    "SCA",
                    f"âœ… Plan created: {plan_id}",
                    {"total_steps": plan_data.get("total_steps", 0)}
                )
                
                return {
                    "status": "completed",
                    "agent": "SCA",
                    "plan_id": plan_id,
                    "plan_title": plan_title,
                    "intervention_plan": plan_data,
                    "total_steps": plan_data.get("total_steps", 0),
                    "actions_taken": [f"Created intervention plan: {plan_title} (ID: {plan_id})"],
                }
            
            except Exception as e:
                logger.error(f"âŒ Failed to create intervention plan: {e}", exc_info=True)
                return {
                    "status": "failed",
                    "agent": "SCA",
                    "error": str(e),
                    "resources": [],
                }
        
        else:
            # Unknown tool
            logger.error(f"âŒ Unknown tool requested: {tool_name}")
            return {
                "status": "failed",
                "error": f"Unknown tool: {tool_name}",
            }
    
    async def _get_conversation_history(self, session_id: str) -> List[Dict[str, str]]:
        """
        Get recent conversation history from Redis.
        
        Args:
            session_id: Session identifier
        
        Returns:
            List of conversation messages (role + content)
        """
        try:
            import json
            redis_client = await get_redis_client()
            
            cache_key = f"conv_history:{session_id}"
            history_json = await redis_client.get(cache_key)
            
            if history_json:
                history = json.loads(history_json)
                logger.debug(f"ðŸ“œ Retrieved {len(history)} messages from history cache")
                return history
            else:
                logger.debug(f"ðŸ“œ No cached history found for session {session_id}")
                return []
                
        except Exception as e:
            logger.warning(f"Failed to get conversation history: {e}")
            return []
    
    async def _save_conversation_history(
        self, 
        session_id: str, 
        user_msg: str, 
        ai_msg: str
    ) -> None:
        """
        Save conversation to Redis with 1-hour TTL.
        
        Maintains sliding window of last 10 messages to keep context manageable.
        
        Args:
            session_id: Session identifier
            user_msg: User's message
            ai_msg: AI's response
        """
        try:
            import json
            redis_client = await get_redis_client()
            
            # Get existing history
            history = await self._get_conversation_history(session_id)
            
            # Append new messages
            history.append({"role": "user", "content": user_msg})
            history.append({"role": "model", "content": ai_msg})
            
            # Keep only last 10 messages (5 turns)
            history = history[-10:]
            
            # Save with 1-hour TTL
            cache_key = f"conv_history:{session_id}"
            await redis_client.setex(
                cache_key,
                3600,  # 1 hour
                json.dumps(history)
            )
            
            logger.debug(f"ðŸ’¾ Saved conversation to cache ({len(history)} messages)")
            
        except Exception as e:
            logger.warning(f"Failed to save conversation history: {e}")
            # Non-critical, continue without caching
