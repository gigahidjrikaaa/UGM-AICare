services:
  #################
  # Database (PostgreSQL)
  #################
  db:
    image: postgres:16-alpine
    container_name: ugm_aicare_db_dev
    environment:
      POSTGRES_DB: ${POSTGRES_DB}
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
    volumes:
      - pgdata_dev:/var/lib/postgresql/data
    ports:
      - "${POSTGRES_PORT:-5432}:5432"
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -h localhost -U ${POSTGRES_USER} -d ${POSTGRES_DB}"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - internal_network_dev

  #################
  # Cache (Redis)
  #################
  redis:
    image: redis/redis-stack-server:latest
    container_name: ugm_aicare_redis_dev
    volumes:
      - redisdata_dev:/data
    ports:
      - "${REDIS_PORT:-6379}:6379"
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - internal_network_dev

  #################
  # Object Storage (MinIO)
  #################
  minio:
    image: minio/minio:RELEASE.2025-09-07T16-13-09Z
    container_name: ugm_aicare_minio_dev
    environment:
      MINIO_ROOT_USER: ${MINIO_ACCESS_KEY:-minioadmin}
      MINIO_ROOT_PASSWORD: ${MINIO_SECRET_KEY:-minioadmin}
    command: server /data --console-address ":9001"
    volumes:
      - minio_data_dev:/data
    ports:
      - "${MINIO_PORT:-9000}:9000"
      - "${MINIO_CONSOLE_PORT:-9001}:9001"
    restart: unless-stopped
    networks:
      - internal_network_dev

  #################
  # Migration Service
  #################
  migrate:
    container_name: ugm_aicare_migrate_dev
    build:
      context: ../../backend
      dockerfile: app/Dockerfile.migrate
    entrypoint: >
      bash -c "
        set -e
        echo '[migrate] Waiting for database...'
        /app/scripts/wait-for-it.sh db:5432 -t 60 -- echo '[migrate] Database is ready!'

        echo '[migrate] Running Alembic migrations...'
        if alembic upgrade head; then
          echo '[migrate] ✓ Migrations completed successfully'
          exit 0
        fi

        echo '[migrate] ⚠ Initial migration attempt failed'

        CURRENT_REV=$$(psql -t -A postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@db:5432/${POSTGRES_DB} -c \"SELECT version_num FROM alembic_version LIMIT 1\" 2>/dev/null || echo '')
        if [ -n \"$$CURRENT_REV\" ]; then
          echo \"[migrate] Current revision: $$CURRENT_REV\"
          if ! grep -r \"revision = '$$CURRENT_REV'\" /app/alembic/versions/ >/dev/null 2>&1; then
            echo '[migrate] ⚠ Revision not found in migration files - stamping to head'
            alembic stamp head
            echo '[migrate] ✓ Database stamped at head'
            exit 0
          fi
        fi

        echo '[migrate] Attempting fresh migration...'
        psql postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@db:5432/${POSTGRES_DB} -c 'TRUNCATE TABLE alembic_version;' 2>/dev/null || true

        if alembic upgrade head; then
          echo '[migrate] ✓ Migrations completed after retry'
          exit 0
        fi

        echo '[migrate] ✗ Migration failed after all attempts'
        exit 1
      "
    env_file:
      - ../../.env
    environment:
      PYTHONPATH: /app
      DATABASE_URL: postgresql+asyncpg://${POSTGRES_USER}:${POSTGRES_PASSWORD}@db:5432/${POSTGRES_DB}
    depends_on:
      db:
        condition: service_healthy
    networks:
      - internal_network_dev

  #################
  # Backend (FastAPI)
  #################
  backend:
    container_name: ugm_aicare_backend_dev
    build:
      context: ../../backend
      dockerfile: app/Dockerfile
    command: >
      bash -c "
        echo 'Starting backend with hot reload...' &&
        uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload --reload-dir /app/app --log-level info
      "
    ports:
      - "8000:8000"
    deploy:
      resources:
        limits:
          memory: 2G
        reservations:
          memory: 1G
    env_file:
      - ../../.env
    environment:
      DATABASE_URL: postgresql+asyncpg://${POSTGRES_USER}:${POSTGRES_PASSWORD}@db:5432/${POSTGRES_DB}
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}

      ADMIN_EMAIL: ${ADMIN_EMAIL}
      ADMIN_PASSWORD: ${ADMIN_PASSWORD}
      COUNSELOR_EMAIL: ${COUNSELOR_EMAIL}
      COUNSELOR_PASSWORD: ${COUNSELOR_PASSWORD}
      COUNSELOR_NAME: ${COUNSELOR_NAME:-Main Counselor}

      FRONTEND_URL: ${FRONTEND_URL}
      BACKEND_URL: ${BACKEND_URL}
      ALLOWED_ORIGINS: "${ALLOWED_ORIGINS:-http://localhost:4000,http://127.0.0.1:4000,http://localhost:3000,http://127.0.0.1:3000}"

      REDIS_HOST: redis
      REDIS_PORT: 6379
      REDIS_USERNAME: ""
      REDIS_PASSWORD: ""
      REDIS_URL: ""

      APP_ENV: development
      PYTHONPATH: /app

      MINIO_ENDPOINT: ${MINIO_ENDPOINT:-minio:9000}
      MINIO_ACCESS_KEY: ${MINIO_ACCESS_KEY:-minioadmin}
      MINIO_SECRET_KEY: ${MINIO_SECRET_KEY:-minioadmin}
      MINIO_BUCKET: ${MINIO_BUCKET:-content-resources}
      MINIO_SECURE: ${MINIO_SECURE:-false}

      JWT_SECRET_KEY: ${JWT_SECRET_KEY}
      EMAIL_ENCRYPTION_KEY: ${EMAIL_ENCRYPTION_KEY}

      LANGFUSE_ENABLED: "false"
      LANGFUSE_HOST: "http://langfuse-server:3000"
    volumes:
      - ../../backend:/app
      - /app/.venv
      - /app/__pycache__
    depends_on:
      migrate:
        condition: service_completed_successfully
      db:
        condition: service_healthy
      redis:
        condition: service_healthy
      minio:
        condition: service_started
    restart: unless-stopped
    networks:
      - internal_network_dev
    dns: 8.8.8.8

  #################
  # Frontend (Next.js)
  #################
  frontend:
    container_name: ugm_aicare_frontend_dev
    build:
      context: ../../frontend
      dockerfile: Dockerfile
      target: development
      args:
        NEXT_PUBLIC_API_URL: ${NEXT_PUBLIC_API_URL}
    ports:
      - "4000:3000"
    deploy:
      resources:
        limits:
          memory: 4G
        reservations:
          memory: 2G
    env_file:
      - ../../.env
    environment:
      NODE_ENV: development
      PORT: 3000
      NEXT_PUBLIC_API_URL: ${NEXT_PUBLIC_API_URL}
      GOOGLE_CLIENT_ID: ${GOOGLE_CLIENT_ID}
      GOOGLE_CLIENT_SECRET: ${GOOGLE_CLIENT_SECRET}
      INTERNAL_API_URL: http://backend:8000
    volumes:
      - ../../frontend:/app
      - /app/node_modules
    depends_on:
      - backend
    restart: unless-stopped
    networks:
      - internal_network_dev

  #################
  # Monitoring Stack (optional)
  #################
  prometheus:
    profiles: ["monitoring"]
    build:
      context: ../monitoring/prometheus
      dockerfile: Dockerfile
    container_name: ugm_aicare_prometheus_dev
    user: root
    ports:
      - "8255:9090"
    volumes:
      - prometheus-data-dev:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=15d'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
      - '--web.enable-lifecycle'
    networks:
      - monitoring
      - internal_network_dev
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:9090/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    restart: unless-stopped

  grafana:
    profiles: ["monitoring"]
    image: grafana/grafana:10.2.2
    container_name: ugm_aicare_grafana_dev
    ports:
      - "8256:3000"
    environment:
      GF_SECURITY_ADMIN_USER: ${GRAFANA_ADMIN_USER:-admin}
      GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_ADMIN_PASSWORD:-admin}
      GF_USERS_ALLOW_SIGN_UP: "false"
      GF_SERVER_ROOT_URL: ${GRAFANA_ROOT_URL:-http://localhost:8256}
      GF_ANALYTICS_REPORTING_ENABLED: "false"
      GF_ANALYTICS_CHECK_FOR_UPDATES: "false"
    volumes:
      - grafana-data-dev:/var/lib/grafana
      - ../monitoring/grafana/dashboards:/etc/grafana/provisioning/dashboards:ro
      - ../monitoring/grafana/datasources:/etc/grafana/provisioning/datasources:ro
    depends_on:
      - prometheus
    networks:
      - monitoring
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:3000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    restart: unless-stopped

  node-exporter:
    profiles: ["monitoring"]
    image: prom/node-exporter:v1.7.0
    container_name: ugm_aicare_node_exporter_dev
    ports:
      - "8257:9100"
    command:
      - '--path.rootfs=/host'
      - '--path.procfs=/host/proc'
      - '--path.sysfs=/host/sys'
      - '--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)' 
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    networks:
      - monitoring
    restart: unless-stopped

  cadvisor:
    profiles: ["monitoring"]
    image: gcr.io/cadvisor/cadvisor:v0.47.2
    container_name: ugm_aicare_cadvisor_dev
    ports:
      - "8258:8080"
    volumes:
      - /:/rootfs:ro
      - /var/run:/var/run:ro
      - /sys:/sys:ro
      - /var/lib/docker/:/var/lib/docker:ro
      - /dev/disk/:/dev/disk:ro
    privileged: true
    devices:
      - /dev/kmsg
    networks:
      - monitoring
    restart: unless-stopped

  postgres-exporter:
    profiles: ["monitoring"]
    image: prometheuscommunity/postgres-exporter:v0.15.0
    container_name: ugm_aicare_postgres_exporter_dev
    ports:
      - "8259:9187"
    environment:
      DATA_SOURCE_NAME: "postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@db:5432/${POSTGRES_DB}?sslmode=disable"
    networks:
      - monitoring
      - internal_network_dev
    restart: unless-stopped

  redis-exporter:
    profiles: ["monitoring"]
    image: oliver006/redis_exporter:v1.55.0
    container_name: ugm_aicare_redis_exporter_dev
    ports:
      - "8260:9121"
    environment:
      REDIS_ADDR: "redis:6379"
    networks:
      - monitoring
      - internal_network_dev
    restart: unless-stopped

  langfuse-server:
    profiles: ["monitoring"]
    image: ghcr.io/langfuse/langfuse:2
    container_name: ugm_aicare_langfuse_dev
    ports:
      - "8262:3000"
    environment:
      NODE_ENV: production
      DATABASE_URL: "postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@db:5432/langfuse_db?schema=public"
      NEXTAUTH_URL: ${LANGFUSE_PUBLIC_URL:-http://localhost:8262}
      NEXTAUTH_SECRET: ${LANGFUSE_SECRET:-dev-only-change-me-please}
      SALT: ${LANGFUSE_SALT:-dev-only-change-me-too}
      TELEMETRY_ENABLED: "false"
      LANGFUSE_ENABLE_EXPERIMENTAL_FEATURES: "true"
      HOSTNAME: 0.0.0.0
    depends_on:
      - db
    networks:
      - monitoring
      - internal_network_dev
    restart: unless-stopped

  #################
  # Loki Stack (optional)
  #################
  loki:
    profiles: ["loki"]
    image: grafana/loki:2.9.3
    container_name: ugm_aicare_loki_dev
    ports:
      - "3100:3100"
    command: -config.file=/etc/loki/local-config.yaml
    volumes:
      - ../loki/loki-config.yaml:/etc/loki/local-config.yaml:ro
      - loki-data-dev:/loki
    networks:
      - monitoring
    restart: unless-stopped

  promtail:
    profiles: ["loki"]
    image: grafana/promtail:2.9.3
    container_name: ugm_aicare_promtail_dev
    volumes:
      - ../loki/promtail-config.yaml:/etc/promtail/config.yaml:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
    command: -config.file=/etc/promtail/config.yaml
    depends_on:
      - loki
    networks:
      - monitoring
    restart: unless-stopped

  #################
  # ELK Stack (optional)
  #################
  elasticsearch:
    profiles: ["elk"]
    image: docker.elastic.co/elasticsearch/elasticsearch:8.11.0
    container_name: ugm_aicare_elasticsearch_dev
    environment:
      discovery.type: single-node
      ES_JAVA_OPTS: "-Xms1g -Xmx1g"
      xpack.security.enabled: "false"
      xpack.security.http.ssl.enabled: "false"
      http.host: 0.0.0.0
      transport.host: 127.0.0.1
    ports:
      - "8250:9200"
      - "8251:9300"
    volumes:
      - elasticsearch-data-dev:/usr/share/elasticsearch/data
    networks:
      - elk
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:9200/_cluster/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    restart: unless-stopped

  logstash:
    profiles: ["elk"]
    image: docker.elastic.co/logstash/logstash:8.11.0
    container_name: ugm_aicare_logstash_dev
    volumes:
      - ../elk/logstash/pipeline:/usr/share/logstash/pipeline:ro
      - ../elk/logstash/config/logstash.yml:/usr/share/logstash/config/logstash.yml:ro
    ports:
      - "8252:5000/tcp"
      - "8252:5000/udp"
      - "8253:9600"
    environment:
      LS_JAVA_OPTS: "-Xmx512m -Xms512m"
    depends_on:
      elasticsearch:
        condition: service_healthy
    networks:
      - elk
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:9600/_node/stats || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 40s
    restart: unless-stopped

  kibana:
    profiles: ["elk"]
    image: docker.elastic.co/kibana/kibana:8.11.0
    container_name: ugm_aicare_kibana_dev
    ports:
      - "8254:5601"
    environment:
      ELASTICSEARCH_HOSTS: http://elasticsearch:9200
      SERVER_NAME: ugm-aicare-kibana
      SERVER_HOST: "0.0.0.0"
    depends_on:
      elasticsearch:
        condition: service_healthy
    networks:
      - elk
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:5601/api/status || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    restart: unless-stopped

  filebeat:
    profiles: ["elk"]
    image: docker.elastic.co/beats/filebeat:8.11.0
    container_name: ugm_aicare_filebeat_dev
    user: root
    volumes:
      - ../elk/filebeat/filebeat.yml:/usr/share/filebeat/filebeat.yml:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - filebeat-data-dev:/usr/share/filebeat/data
    command: filebeat -e -strict.perms=false
    depends_on:
      logstash:
        condition: service_healthy
    networks:
      - elk
    restart: unless-stopped

volumes:
  pgdata_dev:
  redisdata_dev:
  minio_data_dev:
  prometheus-data-dev:
  grafana-data-dev:
  loki-data-dev:
  elasticsearch-data-dev:
  filebeat-data-dev:

networks:
  internal_network_dev:
    driver: bridge
  monitoring:
    driver: bridge
  elk:
    driver: bridge
